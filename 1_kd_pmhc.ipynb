{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge destillation for pMHC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code applied knowledge detilation to ESM2 fine-tuned model for pMHC binding prediction. It's based on the paper:  <br><br>\n",
    "Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In: Neural Information Processing System Deep Learning Workshop (2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/vicente/projects/pmhc/results/train_esm2_t6_rnn_30epochs/checkpoint-303201/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pre_trained_models/esm2_t6_8M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"BertRnn\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cnn_dropout\": 0.1,\n",
      "  \"cnn_filters\": 512,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 320,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1280,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length\": 50,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.1,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n",
      "loading weights file /home/vicente/projects/pmhc/results/train_esm2_t6_rnn_30epochs/checkpoint-303201/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertRnn.\n",
      "\n",
      "All the weights of BertRnn were initialized from the model checkpoint at /home/vicente/projects/pmhc/results/train_esm2_t6_rnn_30epochs/checkpoint-303201/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertRnn for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load fine-tuned model ESM2_T6\n",
    "from model_utils_bert import BertRnn\n",
    "from transformers import BertConfig\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "\n",
    "path_model = \"/home/vicente/projects/pmhc/results/train_esm2_t6_rnn_30epochs/checkpoint-303201/\"\n",
    "path_test_db = \"/home/vicente/projects/pmhc/dataset/hlab/hlab_test.csv\"\n",
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "config = BertConfig.from_pretrained(path_model, num_labels=2 )\n",
    "model_esm2 = BertRnn.from_pretrained(path_model, config=config)\n",
    "#trainer = Trainer(model = model_esm2, compute_metrics = compute_metrics)\n",
    "#test_dataset = DataSetLoaderBERT(path_test_db, tokenizer_name=\"/home/vicente/projects/pmhc/pre_trained_models/esm2_t6_8M_UR50D\", max_length=seq_length)\n",
    "#predictions, label_ids, metrics = trainer.predict(test_dataset)\n",
    "#print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions ###############################################################\n",
    "\n",
    "print(model_esm2)\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(predictions)\n",
    "\n",
    "df['prediction'] = df.apply(lambda row: ( 0 if row[0] > row[1] else 1 ), axis=1)\n",
    "df['label'] = label_ids\n",
    "df.to_csv(\"esm2_t12_rnn_freeze_acc_steps(2).csv\")\n",
    "\"\"\"\n",
    "###################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
