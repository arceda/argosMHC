{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3118b43e",
   "metadata": {},
   "source": [
    "# Predict with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "325c6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb20c02",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16366545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ were not used when initializing BertForSequenceClassification: ['base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.16.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.26.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.bias', 'base_model.model.rnn.weight_hh_l0', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.21.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.output.dense.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.21.output.dense.weight', 'base_model.model.bert.encoder.layer.14.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.bias', 'base_model.model.rnn.bias_hh_l0_reverse', 'base_model.model.bert.encoder.layer.22.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.output.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.intermediate.dense.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_B.default.weight', 'base_model.model.rnn.weight_hh_l1', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.bias', 'base_model.model.rnn.bias_hh_l1_reverse', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.8.output.dense.bias', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.14.output.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.23.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.19.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.28.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.12.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.output.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.4.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.9.output.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.output.dense.weight', 'base_model.model.bert.encoder.layer.18.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_A.default.weight', 'base_model.model.rnn.weight_hh_l1_reverse', 'base_model.model.bert.encoder.layer.6.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.23.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.5.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.output.dense.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.output.dense.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.9.output.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.dense.bias', 'base_model.model.bert.encoder.layer.24.output.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_ih_l0', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.14.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.output.dense.weight', 'base_model.model.bert.encoder.layer.23.output.dense.bias', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.bias', 'base_model.model.rnn.weight_ih_l1', 'base_model.model.bert.encoder.layer.23.intermediate.dense.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.output.dense.bias', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.output.dense.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.32.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.32.output.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.30.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.output.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.output.dense.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.output.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.8.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.output.dense.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.23.output.dense.weight', 'base_model.model.bert.encoder.layer.19.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.output.dense.weight', 'base_model.model.rnn.weight_ih_l0', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.weight', 'base_model.model.rnn.weight_ih_l0_reverse', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_ih_l0_reverse', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.22.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.30.output.dense.bias', 'base_model.model.bert.encoder.layer.3.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.output.dense.bias', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.output.dense.weight', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.31.intermediate.dense.bias', 'base_model.model.rnn.weight_hh_l0_reverse', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.9.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.22.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.4.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.output.dense.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.position_ids', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.output.dense.bias', 'base_model.model.bert.encoder.layer.26.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.intermediate.dense.weight', 'base_model.model.rnn.bias_ih_l1', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.15.output.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.5.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.25.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.7.output.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.bias', 'base_model.model.rnn.bias_ih_l1_reverse', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.27.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.17.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.weight', 'base_model.model.rnn.bias_hh_l1', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.32.output.dense.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.3.output.dense.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.model.rnn.weight_ih_l1_reverse', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.11.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.output.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.output.dense.bias', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.26.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.31.output.dense.weight', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.output.dense.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.weight', 'base_model.model.rnn.bias_hh_l0', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.output.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ and are newly initialized: ['encoder.layer.30.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.30.attention.self.value.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.28.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.24.output.LayerNorm.weight', 'encoder.layer.28.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.25.attention.self.value.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.32.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.26.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.26.intermediate.dense.weight', 'encoder.layer.26.attention.self.query.bias', 'encoder.layer.21.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.31.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.26.attention.self.key.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.32.attention.self.key.bias', 'encoder.layer.32.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.29.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.30.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.32.intermediate.dense.weight', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.32.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.29.output.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.30.attention.self.query.weight', 'encoder.layer.24.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.30.attention.self.query.bias', 'encoder.layer.26.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.24.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.25.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.24.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.26.output.dense.bias', 'encoder.layer.29.output.LayerNorm.bias', 'encoder.layer.26.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.25.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.29.attention.self.query.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.31.attention.output.LayerNorm.weight', 'encoder.layer.26.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.27.attention.output.LayerNorm.bias', 'encoder.layer.27.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.24.attention.self.key.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.26.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.28.attention.self.value.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.32.output.LayerNorm.weight', 'encoder.layer.29.intermediate.dense.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.25.attention.self.value.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.28.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.32.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.25.intermediate.dense.weight', 'encoder.layer.32.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.24.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.25.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.25.output.dense.weight', 'encoder.layer.28.attention.self.key.weight', 'encoder.layer.28.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.26.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.30.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.27.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.29.output.dense.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.27.attention.self.key.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.29.attention.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.27.output.dense.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.32.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.32.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.30.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.31.attention.self.value.weight', 'encoder.layer.27.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.27.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.30.output.dense.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.31.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.28.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.31.attention.self.key.bias', 'encoder.layer.29.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.31.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.29.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.27.intermediate.dense.weight', 'encoder.layer.28.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.31.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.25.attention.self.key.bias', 'encoder.layer.29.attention.self.value.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.29.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.30.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.30.intermediate.dense.weight', 'encoder.layer.31.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.26.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.27.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.28.intermediate.dense.weight', 'classifier.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.24.intermediate.dense.weight', 'encoder.layer.25.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.28.attention.self.value.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.29.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.32.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.24.intermediate.dense.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.31.attention.self.key.weight', 'encoder.layer.25.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.24.output.dense.weight', 'encoder.layer.30.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.31.output.dense.weight', 'encoder.layer.32.attention.self.key.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.26.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.24.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.32.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.31.intermediate.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.25.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.28.attention.self.query.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.30.attention.self.key.weight', 'encoder.layer.25.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.27.attention.self.query.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.29.attention.self.query.weight', 'encoder.layer.32.attention.self.value.bias', 'encoder.layer.29.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.31.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.25.attention.self.query.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.30.attention.self.key.bias', 'encoder.layer.27.attention.self.value.bias', 'encoder.layer.24.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.24.attention.self.key.weight', 'encoder.layer.27.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.24.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.27.intermediate.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.30.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.31.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.24.attention.self.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.32.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.25.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'classifier.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.28.attention.self.query.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.26.attention.self.key.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.28.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.31.attention.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.28.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.27.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.26.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/\"  # mejor checkpoiunt\n",
    "name_results = \"predictions_esm2_lora_t33_c3\" # \n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)\n",
    "\n",
    "#In case you have added tokens, its recommended to use the PeftModel class rather than AutoModelForCausalLM. The former takes into account resizing the embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d08ead",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a350777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(\"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\", tokenizer_name=\"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D\", max_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2761f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 11, 13, 15, 15, 11, 21, 4, 2, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19, 11,\n",
      "        13, 15, 15, 11, 21,  4,  2,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\ntokenizer \\'input_ids\\': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2],\\ndataloader\\'input_ids\\': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2,  1,  1,  1,  1,  1,  1,  1,  1],\\n\\n \\ntokenizer \\'attention_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\\ndataloader\\'attention_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for sample in test_dataset:\n",
    "#    print(sample)\n",
    "#sample = test_dataset[0]\n",
    "#print(sample)\n",
    "#print(sample['input_ids'])\n",
    "#print( next(iter(test_dataset)) )\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[1])\n",
    "\n",
    "#Genera estas diferencia cuando en el tokenizer no definimos que haga un padding \n",
    "\"\"\"\"\n",
    "tokenizer 'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2],\n",
    "dataloader'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2,  1,  1,  1,  1,  1,  1,  1,  1],\n",
    "\n",
    " \n",
    "tokenizer 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "dataloader'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c4961",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4cbbb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/', vocab_size=33, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'})\n",
      "[[{'label': 'LABEL_0', 'score': 0.3711804747581482}, {'label': 'LABEL_1', 'score': 0.628819465637207}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.3667409121990204}, {'label': 'LABEL_1', 'score': 0.6332590579986572}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.3783145546913147}, {'label': 'LABEL_1', 'score': 0.6216854453086853}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.375162273645401}, {'label': 'LABEL_1', 'score': 0.6248377561569214}]]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer.padding='max_length'\n",
    "#tokenizer.max_length=seq_length\n",
    "print(tokenizer)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\")) # label 1\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\")) # label 1\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYDCEKAFFKM\")) # label 0\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYASDDGSWWD\")) # label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dc2af",
   "metadata": {},
   "source": [
    "# Predict Classic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42390a7f",
   "metadata": {},
   "source": [
    "Este cdigo, evalua el modelo despues del fine-tuning en el conjunto de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "edb1143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline, pipeline, BartForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import set_seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c31c82",
   "metadata": {},
   "source": [
    "Abrimos el modelo y el tokenizer. Tambien definimos la ruta del modelo, el tokenizer (modelo pre entrenado) y la base de datos de testing. Finalmente, definimos el sufijo de de los archivos a generar como salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/\"  # mejor checkpoiunt\n",
    "name_results = \"predictions_esm2_t33_c5\" # \n",
    "#pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t6_8M_UR50D/\"\n",
    "dataset = \"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\"\n",
    "\n",
    "model = BertRnn.from_pretrained(model_name, num_labels=2) # it fail for automodel for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbe1f6",
   "metadata": {},
   "source": [
    "Definimos el DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b33a1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(dataset, tokenizer_name=pre_trained, max_length=seq_length)\n",
    "data_iter = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "print(type(test_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296a47",
   "metadata": {},
   "source": [
    "Prediccion usando el dataloader y segn el batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b6978f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    logit_class_0  logit_class_1  prob_class_0  prob_class_0  prediction  \\\n",
      "0        1.484725      -2.111731      0.973311      0.026689           0   \n",
      "1        0.366563      -0.806652      0.763726      0.236274           0   \n",
      "2       -3.797246       3.086962      0.001023      0.998977           1   \n",
      "3       -4.328579       3.746734      0.000311      0.999689           1   \n",
      "4       -1.433432       0.912185      0.087415      0.912585           1   \n",
      "..            ...            ...           ...           ...         ...   \n",
      "71       1.846199      -2.293438      0.984321      0.015679           0   \n",
      "72      -0.228940      -0.585798      0.588280      0.411720           0   \n",
      "73       1.457081      -2.315464      0.977523      0.022477           0   \n",
      "74       1.226935      -1.907617      0.958296      0.041704           0   \n",
      "75       2.441628      -3.086117      0.996041      0.003959           0   \n",
      "\n",
      "    label  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "..    ...  \n",
      "71      0  \n",
      "72      0  \n",
      "73      0  \n",
      "74      0  \n",
      "75      0  \n",
      "\n",
      "[76 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# data_iter, es un dataLoader, de la base de datos de test, y tiene un batch_size de 16\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "model.eval() # is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation,\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad(): # turn off gradients computation\n",
    "    for i, batch in enumerate(data_iter): # por cada batch        \n",
    "        labels.extend(batch['labels'].numpy())\n",
    "        output = model(batch['input_ids'], batch['attention_mask']) # inference\n",
    "        for row_sample in output.logits: # por cada muestra del batch\n",
    "            logits = row_sample.numpy()\n",
    "            probs = softmax(logits)\n",
    "            predictions.append( [logits[0], logits[1], probs[0], probs[1]] )\n",
    "\n",
    "#print(predictions)\n",
    "df = pd.DataFrame(predictions, columns=[\"logit_class_0\", \"logit_class_1\", \"prob_class_0\", \"prob_class_0\"])\n",
    "df['prediction'] = df.apply(lambda row: ( 0 if row[0] > row[1] else 1 ), axis=1)\n",
    "df['label'] = labels\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96837f7b",
   "metadata": {},
   "source": [
    "Ejemplo de como hacer la Prediccion usando una sola muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "937fe41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output SequenceClassifierOutput(loss=None, logits=tensor([-4.3286,  3.7467]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# algunas muestras\n",
    "\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYRSDTPLIY\" # label 1\n",
    "\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYNSDLVQKY\" # label 1\n",
    "\"YFAMYGEKVAHTHVDTLYLRYHYYTWAVWAYTWYLLAASEAPR\"  # label 0\n",
    "\"YFAMYGEKVAHTHVDTLYLRYHYYTWAVWAYTWYQWSEKVTEE\"  # label 0\n",
    "\n",
    "# el tokenizer devuelve los input_ids y el attention_mask como dos listas\n",
    "sample = tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYNSDLVQKY\", padding='max_length', max_length=seq_length)\n",
    "\n",
    "# convertimos en tensor, debe ser lista de listas\n",
    "ids = torch.IntTensor([sample['input_ids']]) # tensor 2D\n",
    "masks = torch.IntTensor([sample['attention_mask']]) # tensor 2D\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # turn off gradients computation\n",
    "    output = model(ids, masks )   \n",
    "\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7d2aa",
   "metadata": {},
   "source": [
    "# Predict classic with Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a945292",
   "metadata": {},
   "source": [
    "Este ejemplo, tambien hace las predicciones y clcula las mtricas, pero usando el Trainner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d029f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../pre_trained_models/esm2_t6_8M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"BertRnn\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cnn_dropout\": 0.1,\n",
      "  \"cnn_filters\": 512,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 320,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1280,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length\": 50,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.1,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n",
      "loading weights file /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertRnn.\n",
      "\n",
      "All the weights of BertRnn were initialized from the model checkpoint at /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertRnn for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "***** Running Prediction *****\n",
      "  Num examples = 76\n",
      "  Batch size = 8\n",
      "100%|| 10/10 [00:00<00:00, 84.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1  prediction  label\n",
      "0   1.484571 -2.111625           0      1\n",
      "1   0.366120 -0.806223           0      1\n",
      "2  -3.797319  3.086969           1      1\n",
      "3  -4.328716  3.746843           1      1\n",
      "4  -1.433371  0.912052           1      1\n",
      "..       ...       ...         ...    ...\n",
      "71  1.846014 -2.293381           0      0\n",
      "72 -0.229090 -0.585730           0      0\n",
      "73  1.457083 -2.315473           0      0\n",
      "74  1.226967 -1.907715           0      0\n",
      "75  2.441543 -3.086087           0      0\n",
      "\n",
      "[76 rows x 4 columns]\n",
      "{'test_loss': 0.3903467059135437, 'test_auc': 0.8815789473684211, 'test_precision': 0.9393939393939394, 'test_recall': 0.8157894736842105, 'test_f1': 0.8732394366197183, 'test_sn': 0.9393939393939394, 'test_sp': 0.9473684210526315, 'test_accuracy': 0.881578947368421, 'test_mcc': 0.7698512161427534, 'test_runtime': 0.1327, 'test_samples_per_second': 572.638, 'test_steps_per_second': 75.347}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions for HLAB dataset for TAPE\n",
    "\n",
    "# load model\n",
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from transformers import BertConfig\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "\n",
    "model_name = \"/M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/\"  # mejor checkpoiunt\n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t6_8M_UR50D/\"\n",
    "dataset = \"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\"\n",
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "config = BertConfig.from_pretrained(model_name, num_labels=2 )\n",
    "\n",
    "model = Trainer(model = BertRnn.from_pretrained(model_name, config=config), compute_metrics = compute_metrics)\n",
    "test_dataset = DataSetLoaderBERT(dataset, tokenizer_name=pre_trained, max_length=seq_length)\n",
    "predictions, label_ids, metrics = model.predict(test_dataset)\n",
    "\n",
    "df = pd.DataFrame(predictions)\n",
    "\n",
    "df['prediction'] = df.apply(lambda row: ( 0 if row[0] > row[1] else 1 ), axis=1)\n",
    "df['label'] = label_ids\n",
    "print(df)\n",
    "#df.to_csv(name_results + \".csv\")\n",
    "\n",
    "#print(predictions)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
