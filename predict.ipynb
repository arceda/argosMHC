{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3118b43e",
   "metadata": {},
   "source": [
    "# Predict with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "325c6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb20c02",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16366545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ were not used when initializing BertForSequenceClassification: ['base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.16.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.26.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.bias', 'base_model.model.rnn.weight_hh_l0', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.21.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.output.dense.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.21.output.dense.weight', 'base_model.model.bert.encoder.layer.14.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.bias', 'base_model.model.rnn.bias_hh_l0_reverse', 'base_model.model.bert.encoder.layer.22.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.output.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.intermediate.dense.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_B.default.weight', 'base_model.model.rnn.weight_hh_l1', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.bias', 'base_model.model.rnn.bias_hh_l1_reverse', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.8.output.dense.bias', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.14.output.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.23.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.19.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.28.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.12.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.output.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.4.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.9.output.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.output.dense.weight', 'base_model.model.bert.encoder.layer.18.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_A.default.weight', 'base_model.model.rnn.weight_hh_l1_reverse', 'base_model.model.bert.encoder.layer.6.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.23.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.5.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.output.dense.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.output.dense.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.9.output.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.dense.bias', 'base_model.model.bert.encoder.layer.24.output.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_ih_l0', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.14.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.output.dense.weight', 'base_model.model.bert.encoder.layer.23.output.dense.bias', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.bias', 'base_model.model.rnn.weight_ih_l1', 'base_model.model.bert.encoder.layer.23.intermediate.dense.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.output.dense.bias', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.output.dense.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.32.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.32.output.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.30.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.output.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.output.dense.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.output.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.8.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.output.dense.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.23.output.dense.weight', 'base_model.model.bert.encoder.layer.19.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.output.dense.weight', 'base_model.model.rnn.weight_ih_l0', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.weight', 'base_model.model.rnn.weight_ih_l0_reverse', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_ih_l0_reverse', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.22.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.30.output.dense.bias', 'base_model.model.bert.encoder.layer.3.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.output.dense.bias', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.output.dense.weight', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.31.intermediate.dense.bias', 'base_model.model.rnn.weight_hh_l0_reverse', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.9.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.22.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.4.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.output.dense.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.position_ids', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.output.dense.bias', 'base_model.model.bert.encoder.layer.26.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.intermediate.dense.weight', 'base_model.model.rnn.bias_ih_l1', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.15.output.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.5.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.25.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.7.output.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.bias', 'base_model.model.rnn.bias_ih_l1_reverse', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.27.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.17.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.weight', 'base_model.model.rnn.bias_hh_l1', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.32.output.dense.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.3.output.dense.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.model.rnn.weight_ih_l1_reverse', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.11.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.output.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.output.dense.bias', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.26.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.31.output.dense.weight', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.output.dense.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.weight', 'base_model.model.rnn.bias_hh_l0', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.output.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ and are newly initialized: ['encoder.layer.30.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.30.attention.self.value.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.28.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.24.output.LayerNorm.weight', 'encoder.layer.28.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.25.attention.self.value.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.32.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.26.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.26.intermediate.dense.weight', 'encoder.layer.26.attention.self.query.bias', 'encoder.layer.21.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.31.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.26.attention.self.key.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.32.attention.self.key.bias', 'encoder.layer.32.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.29.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.30.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.32.intermediate.dense.weight', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.32.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.29.output.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.30.attention.self.query.weight', 'encoder.layer.24.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.30.attention.self.query.bias', 'encoder.layer.26.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.24.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.25.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.24.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.26.output.dense.bias', 'encoder.layer.29.output.LayerNorm.bias', 'encoder.layer.26.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.25.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.29.attention.self.query.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.31.attention.output.LayerNorm.weight', 'encoder.layer.26.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.27.attention.output.LayerNorm.bias', 'encoder.layer.27.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.24.attention.self.key.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.26.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.28.attention.self.value.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.32.output.LayerNorm.weight', 'encoder.layer.29.intermediate.dense.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.25.attention.self.value.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.28.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.32.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.25.intermediate.dense.weight', 'encoder.layer.32.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.24.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.25.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.25.output.dense.weight', 'encoder.layer.28.attention.self.key.weight', 'encoder.layer.28.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.26.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.30.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.27.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.29.output.dense.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.27.attention.self.key.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.29.attention.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.27.output.dense.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.32.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.32.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.30.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.31.attention.self.value.weight', 'encoder.layer.27.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.27.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.30.output.dense.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.31.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.28.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.31.attention.self.key.bias', 'encoder.layer.29.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.31.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.29.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.27.intermediate.dense.weight', 'encoder.layer.28.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.31.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.25.attention.self.key.bias', 'encoder.layer.29.attention.self.value.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.29.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.30.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.30.intermediate.dense.weight', 'encoder.layer.31.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.26.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.27.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.28.intermediate.dense.weight', 'classifier.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.24.intermediate.dense.weight', 'encoder.layer.25.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.28.attention.self.value.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.29.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.32.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.24.intermediate.dense.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.31.attention.self.key.weight', 'encoder.layer.25.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.24.output.dense.weight', 'encoder.layer.30.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.31.output.dense.weight', 'encoder.layer.32.attention.self.key.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.26.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.24.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.32.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.31.intermediate.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.25.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.28.attention.self.query.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.30.attention.self.key.weight', 'encoder.layer.25.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.27.attention.self.query.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.29.attention.self.query.weight', 'encoder.layer.32.attention.self.value.bias', 'encoder.layer.29.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.31.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.25.attention.self.query.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.30.attention.self.key.bias', 'encoder.layer.27.attention.self.value.bias', 'encoder.layer.24.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.24.attention.self.key.weight', 'encoder.layer.27.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.24.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.27.intermediate.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.30.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.31.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.24.attention.self.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.32.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.25.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'classifier.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.28.attention.self.query.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.26.attention.self.key.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.28.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.31.attention.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.28.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.27.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.26.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/\"  # mejor checkpoiunt\n",
    "name_results = \"predictions_esm2_lora_t33_c3\" # \n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)\n",
    "\n",
    "#In case you have added tokens, it’s recommended to use the PeftModel class rather than AutoModelForCausalLM. The former takes into account resizing the embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d08ead",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a350777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(\"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\", tokenizer_name=\"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D\", max_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2761f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 11, 13, 15, 15, 11, 21, 4, 2, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19, 11,\n",
      "        13, 15, 15, 11, 21,  4,  2,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\ntokenizer \\'input_ids\\': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2],\\ndataloader\\'input_ids\\': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2,  1,  1,  1,  1,  1,  1,  1,  1],\\n\\n \\ntokenizer \\'attention_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\\ndataloader\\'attention_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for sample in test_dataset:\n",
    "#    print(sample)\n",
    "#sample = test_dataset[0]\n",
    "#print(sample)\n",
    "#print(sample['input_ids'])\n",
    "#print( next(iter(test_dataset)) )\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[1])\n",
    "\n",
    "#Genera estas diferencia cuando en el tokenizer no definimos que haga un padding \n",
    "\"\"\"\"\n",
    "tokenizer 'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2],\n",
    "dataloader'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2,  1,  1,  1,  1,  1,  1,  1,  1],\n",
    "\n",
    " \n",
    "tokenizer 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "dataloader'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c4961",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4cbbb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/', vocab_size=33, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'})\n",
      "[[{'label': 'LABEL_0', 'score': 0.3711804747581482}, {'label': 'LABEL_1', 'score': 0.628819465637207}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.3667409121990204}, {'label': 'LABEL_1', 'score': 0.6332590579986572}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.3783145546913147}, {'label': 'LABEL_1', 'score': 0.6216854453086853}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.375162273645401}, {'label': 'LABEL_1', 'score': 0.6248377561569214}]]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer.padding='max_length'\n",
    "#tokenizer.max_length=seq_length\n",
    "print(tokenizer)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\")) # label 1\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\")) # label 1\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYDCEKAFFKM\")) # label 0\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYASDDGSWWD\")) # label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dc2af",
   "metadata": {},
   "source": [
    "# Predict Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edb1143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline, pipeline\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1672458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertRnn(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "      (position_embeddings): Embedding(1026, 1280)\n",
      "      (token_type_embeddings): Embedding(2, 1280)\n",
      "      (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (24): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (25): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (26): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (27): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (28): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (29): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (30): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (31): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (32): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (rnn): LSTM(1280, 768, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1536, out_features=2, bias=True)\n",
      "  (criterion): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/classic_t33_c4/checkpoint-27000/\"  # mejor checkpoiunt\n",
    "name_results = \"predictions_esm2_t33_c5\" # \n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "\n",
    "model = BertRnn.from_pretrained(model_name, num_labels=2) # it fail for automodel for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c95d342e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m19\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/argosMHC/model_utils_bert.py:270\u001b[0m, in \u001b[0;36mBertRnn.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    267\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m#print(input_ids.shape) #[batch_size, max_length] = [16, 50]\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m bert_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# lstm\u001b[39;00m\n\u001b[1;32m    282\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m bert_out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:962\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 962\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input = np.array([0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "print(model(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b33a1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L F G R D L', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y T D K K T H L', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y R S D T P L I Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y N S D L V Q K Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L S D L L D W K', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L L Q N D G F F', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y D S D M Q T L V', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y T D Y H V R V Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y V L D S E G Y L', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y S D F H N N R Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y D K S M V D K Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y Y T D Y L T V M', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y D F A E R H G Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y V L D I P S K Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y T D L T R E V Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y Y T D P E V F K', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y S D I H D F E Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y R D W A H N S L', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y F T S M T R L Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y S D S K I Q K Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L T D T F T A Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y F P G N Y S G Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y F S D V M E D L', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y F L E S T F L K', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y Y I D E Q F E R', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y T D L T R D I Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y D P N S T Q R Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y T P D I K S H Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y P E V Q K K K Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L V D L P E Y Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L T E L P D W S', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L L D S S L E Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y V T D T G A L Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y I V D H I H F Q', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y E L H N Q K G Y', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y F T E I G I H L', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y L V D E W L D S', 'Y F A M Y Q E N M A H T D A N T L Y I I Y R D Y T W V A R V Y R G Y F T D K A A S Y', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y D C E K A F F K M', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y S P P R R N F S P', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y H T V Y Y G G F H', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y V D K T K S V V T', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y T P E V N Q T T L', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y G S E I D C A D K', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y A P P Q I D N G I', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y E T V R K L Q A R', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y L G I P D A V S Y', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y G K C E V L E V S', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y G Y Q T I D D Y Y', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y T I E K H K Q N S', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y F S G N V K V D E', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y S A R S P S S L S', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y I E V L C E N F P', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y W P D P P D L P T', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y D A L E F I G K K', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y N V D Q I L K W I', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y H H A H S D A Q G', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y C E E E I N S T F', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y P Q A Q P H Q V Q', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y E D R E S P S V K', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y G T P P L S T E T', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y R R L M F C Y I S', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y I G N I K T V Q I', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y F V S Q V E S D T', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y L H Q L I K Q T Q', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y S N P F E I F F G', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y Y A P Y P S P V L', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y R P V T T T K R E', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y A S D D G S W W D', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y E N P P V E D S S', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y T S C N S G T Y R', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y V T A N V V D P G', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y K L R K K L K T A', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y P P K K S K D K L', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y L L A A S E A P R', 'Y F A M Y G E K V A H T H V D T L Y L R Y H Y Y T W A V W A Y T W Y Q W S E K V T E E']\n",
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 11, 13, 15, 15, 11, 21, 4, 2, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19, 11,\n",
      "        13, 15, 15, 11, 21,  4,  2,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(\"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\", tokenizer_name=\"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D\", max_length=seq_length)\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2f402af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BertRnn' is not supported for . Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'EsmForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegatronBertForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/', vocab_size=33, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'})\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1536x0 and 1536x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m TextClassificationPipeline(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, return_all_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#pipe = pipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# label 1\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\")) # label 1\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYDCEKAFFKM\")) # label 0\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYASDDGSWWD\")) # label 0\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:140\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:114\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:115\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    114\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 115\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m    989\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 990\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:167\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_inputs):\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/argosMHC/model_utils_bert.py:288\u001b[0m, in \u001b[0;36mBertRnn.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    286\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmax_pool1d(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len)\n\u001b[1;32m    287\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(output\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[0;32m--> 288\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1536x0 and 1536x2)"
     ]
    }
   ],
   "source": [
    "#tokenizer.padding='max_length'\n",
    "#tokenizer.max_length=seq_length\n",
    "print(tokenizer)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "#pipe = pipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "print(pipe([\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\"])) # label 1\n",
    "#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\")) # label 1\n",
    "#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYDCEKAFFKM\")) # label 0\n",
    "#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYASDDGSWWD\")) # label 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
