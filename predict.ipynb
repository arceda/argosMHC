{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ae5baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    auc, roc_curve,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def get_metrics(y_test, y_test_predictions, y_probs):\n",
    "    accuracy = accuracy_score(y_test, y_test_predictions)\n",
    "    precision = precision_score(y_test, y_test_predictions)\n",
    "    recall = recall_score(y_test, y_test_predictions)\n",
    "    f1score = f1_score(y_test, y_test_predictions)\n",
    "    #auc = roc_auc_score(y_test, y_test_predictions)\n",
    "    mcc = matthews_corrcoef(y_test, y_test_predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_predictions)\n",
    "    #auc_val = roc_auc_score(y_test, y_probs)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_probs, pos_label = 1)\n",
    "    auc_val = auc(fpr, tpr)  \n",
    "\n",
    "    return {\"accuracy\":accuracy, \"precision\":precision, \"recall\":recall, \"f1score\":f1score, \"auc\":auc_val, \"mcc\":mcc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3118b43e",
   "metadata": {},
   "source": [
    "# 1 Predict with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0a9f0",
   "metadata": {},
   "source": [
    "This way of prediction using LoRA return always 1 or 0; however, classic ESM2 (BertRnn) model works well with this prediction methodology, returning the same resutls than the trainer (section 3 and 4). Some reasons can be:\n",
    "* We don't have a config.json, but we included the config produce after training in models folder; however, it didn't works as well.\n",
    "* As LoRA update only A and B matrices, Maybe we need to include and additional step before doing inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "325c6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline, pipeline, BartForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import set_seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb20c02",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16366545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ were not used when initializing BertRnn: ['base_model.model.bert.encoder.layer.7.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.bias', 'base_model.model.rnn.bias_hh_l0', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.output.dense.bias', 'base_model.model.bert.encoder.layer.3.output.dense.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.27.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.19.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.bias', 'base_model.model.rnn.bias_ih_l0_reverse', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.output.dense.bias', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.30.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.24.output.dense.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.19.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.output.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.10.output.dense.weight', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_A.default.weight', 'base_model.model.rnn.bias_hh_l1_reverse', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.4.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_ih_l0', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_B.default.weight', 'base_model.model.rnn.weight_hh_l0', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.31.intermediate.dense.bias', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.22.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.18.output.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.14.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.32.output.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.3.output.dense.bias', 'base_model.model.bert.encoder.layer.11.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.9.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.13.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.28.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.output.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.12.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.17.output.dense.bias', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.16.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.6.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.output.dense.weight', 'base_model.model.bert.encoder.layer.11.output.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.22.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.3.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_A.default.weight', 'base_model.model.rnn.bias_ih_l1', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.22.output.dense.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.position_ids', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.output.dense.weight', 'base_model.model.rnn.weight_hh_l1', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.4.output.dense.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.9.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.32.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.11.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.15.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.15.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.weight', 'base_model.model.rnn.bias_hh_l1', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.output.dense.weight', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.25.output.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.output.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.29.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.4.output.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.23.output.dense.bias', 'base_model.model.bert.encoder.layer.13.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.9.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.bias', 'base_model.model.rnn.weight_hh_l0_reverse', 'base_model.model.bert.encoder.layer.28.output.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.output.dense.bias', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_A.default.weight', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.26.output.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.output.dense.bias', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.32.output.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.3.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.17.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.23.output.dense.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.weight', 'base_model.model.rnn.weight_ih_l0_reverse', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.bias', 'base_model.model.rnn.weight_ih_l1_reverse', 'base_model.model.bert.encoder.layer.21.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.7.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.24.output.dense.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.rnn.weight_ih_l1', 'base_model.model.bert.encoder.layer.17.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.8.output.dense.weight', 'base_model.model.rnn.weight_hh_l1_reverse', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.21.output.dense.weight', 'base_model.model.bert.encoder.layer.18.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.13.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.22.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_hh_l0_reverse', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.18.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.21.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.30.output.dense.bias', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.bias', 'base_model.model.rnn.bias_ih_l1_reverse', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.12.output.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.18.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.31.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.bias', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_B.default.weight', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.8.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.30.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.10.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.5.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.output.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.output.dense.weight', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.23.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.18.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.6.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.25.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.17.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.19.output.dense.bias', 'base_model.model.bert.encoder.layer.16.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.9.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.weight', 'base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.30.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.29.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.weight', 'base_model.model.rnn.weight_ih_l0', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.output.dense.bias', 'base_model.model.bert.encoder.layer.13.output.dense.bias', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_A.default.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.11.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.6.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.31.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.6.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.21.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.27.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.20.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.output.dense.bias', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_B.default.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.14.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.6.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.output.dense.weight', 'base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.14.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.15.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.bias']\n",
      "- This IS expected if you are initializing BertRnn from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertRnn from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertRnn were not initialized from the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ and are newly initialized: ['encoder.layer.15.intermediate.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.30.output.dense.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.27.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.32.output.LayerNorm.bias', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.26.attention.output.LayerNorm.bias', 'encoder.layer.15.output.dense.weight', 'encoder.layer.28.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.27.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.29.attention.output.LayerNorm.weight', 'encoder.layer.24.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.32.attention.self.key.weight', 'classifier.bias', 'encoder.layer.27.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.32.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.32.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.31.attention.self.key.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.24.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.24.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.25.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.32.intermediate.dense.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.27.output.LayerNorm.weight', 'encoder.layer.28.attention.self.query.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.29.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'rnn.bias_hh_l0', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.30.attention.self.key.weight', 'encoder.layer.27.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.12.output.dense.weight', 'pooler.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.29.attention.self.key.weight', 'encoder.layer.29.attention.self.value.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.26.output.dense.weight', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.29.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.29.attention.self.query.bias', 'rnn.bias_ih_l0', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.31.attention.self.key.bias', 'encoder.layer.30.output.LayerNorm.weight', 'rnn.weight_hh_l0', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.31.attention.self.query.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.30.attention.self.value.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.29.attention.self.query.weight', 'encoder.layer.25.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.24.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.30.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.26.attention.self.key.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.25.attention.self.query.weight', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.32.attention.self.value.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.12.output.dense.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.24.attention.self.value.bias', 'encoder.layer.30.attention.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.28.intermediate.dense.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.30.attention.self.value.weight', 'encoder.layer.32.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.27.intermediate.dense.bias', 'encoder.layer.32.attention.output.dense.bias', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.31.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.30.intermediate.dense.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.25.output.dense.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'rnn.bias_hh_l0_reverse', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.30.output.dense.weight', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.19.attention.self.value.weight', 'rnn.bias_hh_l1_reverse', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.19.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.31.output.LayerNorm.bias', 'encoder.layer.26.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.31.attention.self.value.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.26.output.LayerNorm.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.28.output.dense.weight', 'encoder.layer.31.intermediate.dense.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.24.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.25.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.28.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'rnn.weight_hh_l1_reverse', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.28.attention.output.LayerNorm.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.27.output.LayerNorm.bias', 'encoder.layer.25.output.LayerNorm.weight', 'encoder.layer.26.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'rnn.weight_hh_l0_reverse', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.28.attention.self.key.bias', 'encoder.layer.29.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.31.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.18.output.dense.bias', 'encoder.layer.26.attention.self.query.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.15.output.dense.bias', 'encoder.layer.25.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.24.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.32.attention.self.query.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.27.attention.self.query.weight', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.28.output.LayerNorm.weight', 'rnn.bias_hh_l1', 'encoder.layer.31.output.dense.bias', 'encoder.layer.27.attention.self.query.bias', 'encoder.layer.32.attention.output.LayerNorm.bias', 'encoder.layer.24.attention.self.key.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.25.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.26.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.31.attention.self.query.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.25.attention.self.key.bias', 'encoder.layer.29.output.dense.weight', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.31.attention.self.value.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.25.attention.self.key.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.27.intermediate.dense.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.28.intermediate.dense.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.31.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.24.attention.self.value.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'rnn.weight_ih_l0_reverse', 'encoder.layer.26.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.25.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.29.output.LayerNorm.weight', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.24.output.dense.bias', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.26.attention.self.value.bias', 'rnn.bias_ih_l0_reverse', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.32.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.31.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.29.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.24.output.dense.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.30.attention.self.key.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.12.attention.self.value.bias', 'rnn.weight_hh_l1', 'encoder.layer.24.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.30.intermediate.dense.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.29.intermediate.dense.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.29.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.29.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'rnn.weight_ih_l1', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.31.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.28.attention.self.value.weight', 'encoder.layer.28.attention.output.LayerNorm.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.24.attention.self.query.weight', 'encoder.layer.27.attention.self.value.weight', 'encoder.layer.32.attention.self.key.bias', 'encoder.layer.21.attention.output.dense.bias', 'classifier.weight', 'encoder.layer.28.output.LayerNorm.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.28.output.dense.bias', 'rnn.weight_ih_l0', 'encoder.layer.9.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.26.attention.self.value.weight', 'encoder.layer.30.attention.self.query.bias', 'encoder.layer.25.attention.self.value.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.25.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.32.attention.self.query.bias', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.30.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.26.intermediate.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.13.output.LayerNorm.weight', 'rnn.weight_ih_l1_reverse', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.32.attention.self.value.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.26.attention.self.key.weight', 'encoder.layer.30.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.25.attention.output.LayerNorm.weight', 'encoder.layer.32.output.dense.weight', 'encoder.layer.24.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.27.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.27.attention.self.key.weight', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.27.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.17.intermediate.dense.bias', 'rnn.bias_ih_l1_reverse', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.28.attention.self.key.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.16.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.26.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'rnn.bias_ih_l1', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.19.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/\"  # mejor heckpoiunt\n",
    "name_results = \"predictions_esm2_lora_t33_c3\" # \n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "dataset = \"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\"\n",
    "\n",
    "model = BertRnn.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)\n",
    "\n",
    "#In case you have added tokens, its recommended to use the PeftModel class rather than AutoModelForCausalLM. The former takes into account resizing the embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d08ead",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a350777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(dataset, tokenizer_name=pre_trained, max_length=seq_length)\n",
    "data_iter = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "print(type(test_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bcfeba",
   "metadata": {},
   "source": [
    "Prediction, it always returns zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a71cd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    logit_class_0  logit_class_1  prob_class_0  prob_class_1  prediction  \\\n",
      "0        0.047956      -0.010227      0.514542      0.485458           0   \n",
      "1        0.040884       0.008847      0.508009      0.491991           0   \n",
      "2        0.037705       0.000298      0.509351      0.490649           0   \n",
      "3        0.038920      -0.005073      0.510996      0.489004           0   \n",
      "4        0.043267      -0.009668      0.513231      0.486769           0   \n",
      "..            ...            ...           ...           ...         ...   \n",
      "71       0.030311      -0.001709      0.508004      0.491996           0   \n",
      "72       0.048248       0.005792      0.510612      0.489388           0   \n",
      "73       0.039447       0.020678      0.504692      0.495308           0   \n",
      "74       0.045509      -0.007176      0.513168      0.486832           0   \n",
      "75       0.043361       0.008816      0.508635      0.491365           0   \n",
      "\n",
      "    label  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "..    ...  \n",
      "71      0  \n",
      "72      0  \n",
      "73      0  \n",
      "74      0  \n",
      "75      0  \n",
      "\n",
      "[76 rows x 6 columns]\n",
      "{'accuracy': 0.5, 'precision': 0.0, 'recall': 0.0, 'f1score': 0.0, 'auc': 0.29986149584487537, 'mcc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# data_iter, es un dataLoader, de la base de datos de test, y tiene un batch_size de 16\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "model.eval() # is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation,\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad(): # turn off gradients computation\n",
    "    for i, batch in enumerate(data_iter): # por cada batch        \n",
    "        labels.extend(batch['labels'].numpy())\n",
    "        output = model(batch['input_ids'], batch['attention_mask']) # inference\n",
    "        for row_sample in output.logits: # por cada muestra del batch\n",
    "            logits = row_sample.numpy()\n",
    "            probs = softmax(logits)\n",
    "            predictions.append( [logits[0], logits[1], probs[0], probs[1]] )\n",
    "\n",
    "#print(predictions)\n",
    "df = pd.DataFrame(predictions, columns=[\"logit_class_0\", \"logit_class_1\", \"prob_class_0\", \"prob_class_1\"])\n",
    "df['prediction'] = df.apply(lambda row: ( 0 if row[0] > row[1] else 1 ), axis=1)\n",
    "df['label'] = labels\n",
    "print(df)\n",
    "\n",
    "print(get_metrics(df['label'], df['prediction'], df['prob_class_1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fa4e8",
   "metadata": {},
   "source": [
    "# 2 Predict LoRA using Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ca9aa",
   "metadata": {},
   "source": [
    "Lets evaluate if Trainer give the same results than prediction (section 1). We will evaluate that, because during training, the model got good results on evaluation dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970661f",
   "metadata": {},
   "source": [
    "After evaluation, the model performed well; thusm we are missing a step in section 1 (predict with LoRa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124c49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type esm to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at /M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/ were not used when initializing BertRnn: ['esm.encoder.layer.14.LayerNorm.weight', 'esm.encoder.layer.16.attention.LayerNorm.bias', 'esm.encoder.layer.5.attention.self.key.weight', 'esm.encoder.layer.19.attention.LayerNorm.weight', 'esm.encoder.layer.7.attention.self.value.bias', 'esm.encoder.layer.32.intermediate.dense.bias', 'esm.encoder.layer.30.LayerNorm.weight', 'esm.encoder.layer.7.LayerNorm.bias', 'esm.encoder.layer.6.output.dense.weight', 'esm.encoder.layer.28.intermediate.dense.bias', 'esm.encoder.layer.14.intermediate.dense.bias', 'esm.encoder.layer.8.attention.self.query.bias', 'esm.encoder.layer.27.attention.self.key.weight', 'esm.encoder.layer.3.attention.self.value.weight', 'esm.encoder.layer.6.attention.output.dense.bias', 'esm.encoder.layer.6.LayerNorm.weight', 'esm.encoder.layer.10.attention.LayerNorm.weight', 'esm.encoder.layer.21.attention.output.dense.weight', 'esm.encoder.layer.3.intermediate.dense.weight', 'esm.encoder.layer.21.attention.self.value.weight', 'esm.encoder.layer.10.attention.self.value.weight', 'esm.encoder.layer.28.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.17.intermediate.dense.bias', 'esm.encoder.layer.18.attention.output.dense.weight', 'esm.encoder.layer.15.attention.self.value.bias', 'esm.encoder.layer.0.attention.LayerNorm.bias', 'esm.encoder.layer.24.attention.LayerNorm.weight', 'esm.encoder.layer.24.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.27.attention.output.dense.weight', 'esm.encoder.layer.0.attention.self.value.bias', 'esm.encoder.layer.4.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.10.intermediate.dense.bias', 'esm.encoder.layer.3.attention.self.query.bias', 'esm.encoder.layer.29.LayerNorm.bias', 'esm.encoder.layer.32.output.dense.weight', 'esm.encoder.layer.20.attention.LayerNorm.bias', 'esm.encoder.layer.19.intermediate.dense.bias', 'esm.encoder.layer.15.output.dense.weight', 'esm.encoder.layer.20.attention.output.dense.weight', 'esm.encoder.layer.31.LayerNorm.weight', 'esm.encoder.layer.14.attention.self.query.weight', 'esm.encoder.layer.17.attention.self.query.bias', 'esm.encoder.layer.18.attention.self.key.weight', 'esm.encoder.layer.4.LayerNorm.bias', 'esm.encoder.layer.26.LayerNorm.weight', 'esm.encoder.layer.14.attention.self.value.bias', 'esm.encoder.layer.7.intermediate.dense.bias', 'esm.encoder.layer.30.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.22.LayerNorm.weight', 'esm.encoder.layer.1.intermediate.dense.bias', 'esm.encoder.layer.12.attention.self.value.bias', 'esm.encoder.layer.26.attention.output.dense.weight', 'esm.encoder.layer.26.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.18.attention.self.key.bias', 'esm.encoder.layer.28.attention.self.value.bias', 'esm.encoder.layer.31.intermediate.dense.weight', 'esm.encoder.layer.8.attention.self.value.bias', 'esm.encoder.layer.11.attention.self.key.bias', 'esm.encoder.layer.1.attention.self.query.bias', 'esm.encoder.layer.23.attention.self.key.weight', 'esm.encoder.layer.19.attention.self.query.bias', 'esm.encoder.layer.25.LayerNorm.bias', 'esm.encoder.layer.31.output.dense.bias', 'esm.encoder.layer.26.intermediate.dense.weight', 'esm.embeddings.word_embeddings.weight', 'esm.encoder.layer.17.attention.LayerNorm.weight', 'esm.encoder.layer.7.attention.output.dense.bias', 'esm.encoder.layer.13.intermediate.dense.bias', 'esm.encoder.layer.31.attention.self.key.bias', 'esm.encoder.layer.29.intermediate.dense.bias', 'esm.encoder.layer.10.LayerNorm.weight', 'esm.encoder.layer.12.attention.output.dense.weight', 'esm.encoder.layer.3.attention.output.dense.bias', 'esm.encoder.layer.3.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.30.attention.output.dense.bias', 'esm.encoder.layer.7.attention.self.query.weight', 'esm.encoder.layer.25.intermediate.dense.weight', 'esm.encoder.layer.7.attention.self.key.bias', 'esm.encoder.layer.26.output.dense.bias', 'esm.encoder.layer.29.attention.LayerNorm.bias', 'esm.encoder.layer.15.LayerNorm.weight', 'esm.encoder.layer.0.attention.self.query.weight', 'esm.encoder.layer.15.attention.self.key.bias', 'esm.encoder.layer.31.intermediate.dense.bias', 'esm.encoder.layer.22.attention.output.dense.weight', 'esm.encoder.layer.7.attention.LayerNorm.weight', 'esm.encoder.layer.10.attention.self.query.bias', 'esm.encoder.layer.4.attention.self.key.weight', 'esm.encoder.layer.6.output.dense.bias', 'esm.encoder.layer.31.attention.self.query.weight', 'esm.encoder.layer.20.attention.self.value.weight', 'esm.encoder.layer.17.attention.self.key.bias', 'esm.encoder.layer.7.attention.LayerNorm.bias', 'esm.encoder.layer.18.LayerNorm.bias', 'esm.encoder.layer.19.LayerNorm.bias', 'esm.encoder.layer.21.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.9.attention.output.dense.weight', 'esm.encoder.layer.32.attention.output.dense.weight', 'esm.encoder.layer.20.attention.self.key.weight', 'esm.encoder.layer.8.LayerNorm.bias', 'esm.encoder.layer.25.output.dense.weight', 'esm.encoder.layer.25.attention.self.key.bias', 'esm.embeddings.position_embeddings.weight', 'esm.encoder.layer.27.output.dense.bias', 'esm.encoder.layer.28.attention.output.dense.bias', 'esm.encoder.layer.4.LayerNorm.weight', 'esm.encoder.layer.23.output.dense.weight', 'esm.encoder.layer.2.output.dense.weight', 'esm.encoder.layer.13.LayerNorm.weight', 'esm.encoder.layer.15.attention.self.query.bias', 'esm.encoder.layer.29.attention.self.query.bias', 'esm.encoder.layer.2.LayerNorm.weight', 'esm.encoder.layer.6.intermediate.dense.bias', 'esm.encoder.layer.24.attention.self.query.weight', 'esm.encoder.layer.5.attention.self.query.weight', 'esm.encoder.layer.21.LayerNorm.weight', 'esm.encoder.layer.2.intermediate.dense.weight', 'esm.encoder.layer.8.attention.self.key.weight', 'esm.encoder.layer.21.attention.self.key.weight', 'esm.encoder.layer.12.attention.LayerNorm.bias', 'esm.encoder.layer.6.attention.self.query.bias', 'esm.encoder.layer.7.LayerNorm.weight', 'esm.encoder.layer.18.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.5.intermediate.dense.weight', 'esm.encoder.layer.12.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.12.output.dense.weight', 'esm.encoder.layer.15.attention.output.dense.bias', 'esm.encoder.layer.22.output.dense.weight', 'esm.encoder.layer.32.attention.self.value.weight', 'esm.encoder.layer.6.attention.output.dense.weight', 'esm.encoder.layer.23.LayerNorm.weight', 'esm.encoder.layer.13.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.0.intermediate.dense.weight', 'esm.encoder.layer.8.attention.output.dense.weight', 'esm.encoder.layer.1.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.22.attention.LayerNorm.weight', 'esm.encoder.layer.27.LayerNorm.bias', 'esm.encoder.layer.17.LayerNorm.weight', 'esm.encoder.layer.16.attention.self.value.weight', 'esm.encoder.layer.29.output.dense.weight', 'esm.encoder.layer.30.attention.self.query.weight', 'esm.encoder.layer.9.attention.self.value.bias', 'esm.encoder.layer.21.attention.LayerNorm.weight', 'esm.encoder.layer.13.attention.output.dense.bias', 'esm.encoder.layer.23.output.dense.bias', 'esm.encoder.layer.11.attention.self.query.weight', 'esm.encoder.layer.23.attention.self.key.bias', 'esm.encoder.layer.1.attention.self.value.weight', 'esm.encoder.layer.21.LayerNorm.bias', 'esm.encoder.layer.21.intermediate.dense.weight', 'esm.encoder.layer.23.attention.LayerNorm.bias', 'esm.encoder.layer.31.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.1.attention.output.dense.bias', 'esm.encoder.layer.16.attention.output.dense.bias', 'esm.encoder.layer.32.attention.self.query.bias', 'esm.encoder.layer.5.attention.self.key.bias', 'esm.encoder.layer.16.LayerNorm.bias', 'esm.encoder.layer.29.attention.output.dense.bias', 'esm.encoder.layer.2.intermediate.dense.bias', 'esm.encoder.layer.16.attention.self.key.bias', 'esm.encoder.layer.3.attention.self.query.weight', 'esm.encoder.layer.7.attention.output.dense.weight', 'esm.encoder.layer.24.intermediate.dense.bias', 'esm.encoder.layer.27.intermediate.dense.weight', 'esm.encoder.layer.28.intermediate.dense.weight', 'esm.encoder.layer.25.attention.self.query.bias', 'esm.encoder.layer.12.attention.self.value.weight', 'esm.encoder.layer.0.attention.output.dense.weight', 'esm.encoder.layer.10.LayerNorm.bias', 'esm.encoder.layer.25.attention.self.value.bias', 'esm.encoder.layer.32.attention.self.key.weight', 'esm.encoder.layer.8.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.10.intermediate.dense.weight', 'esm.encoder.layer.8.intermediate.dense.bias', 'esm.encoder.layer.8.output.dense.bias', 'esm.encoder.layer.4.intermediate.dense.bias', 'esm.encoder.layer.0.attention.self.query.bias', 'esm.encoder.layer.2.attention.LayerNorm.bias', 'esm.encoder.layer.12.LayerNorm.weight', 'esm.encoder.layer.4.attention.self.value.bias', 'esm.encoder.layer.25.attention.output.dense.weight', 'esm.encoder.layer.4.output.dense.bias', 'esm.encoder.layer.24.intermediate.dense.weight', 'esm.encoder.layer.32.attention.output.dense.bias', 'esm.encoder.layer.16.intermediate.dense.bias', 'esm.encoder.layer.26.attention.LayerNorm.weight', 'esm.encoder.layer.28.attention.LayerNorm.weight', 'esm.encoder.layer.24.LayerNorm.bias', 'esm.encoder.layer.16.attention.output.dense.weight', 'esm.encoder.layer.9.attention.LayerNorm.bias', 'esm.encoder.layer.17.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.12.attention.LayerNorm.weight', 'esm.encoder.layer.22.attention.self.query.weight', 'esm.encoder.layer.32.LayerNorm.bias', 'esm.encoder.layer.25.attention.output.dense.bias', 'esm.encoder.layer.15.attention.self.key.weight', 'esm.encoder.layer.9.intermediate.dense.bias', 'esm.encoder.layer.0.output.dense.weight', 'esm.encoder.layer.19.LayerNorm.weight', 'esm.encoder.layer.3.attention.self.key.weight', 'esm.encoder.layer.17.attention.self.query.weight', 'esm.encoder.layer.13.attention.self.value.weight', 'esm.encoder.layer.2.attention.self.value.weight', 'esm.encoder.layer.23.intermediate.dense.weight', 'esm.encoder.layer.7.attention.self.query.bias', 'esm.encoder.layer.27.attention.self.value.bias', 'esm.encoder.layer.24.attention.self.key.weight', 'esm.encoder.layer.16.attention.LayerNorm.weight', 'esm.encoder.layer.19.intermediate.dense.weight', 'esm.encoder.layer.20.attention.self.key.bias', 'esm.encoder.layer.32.attention.self.value.bias', 'esm.encoder.layer.13.output.dense.bias', 'esm.encoder.layer.20.intermediate.dense.weight', 'esm.encoder.layer.28.attention.LayerNorm.bias', 'esm.encoder.layer.22.intermediate.dense.bias', 'esm.encoder.layer.23.intermediate.dense.bias', 'esm.encoder.layer.25.attention.self.query.weight', 'esm.encoder.layer.8.LayerNorm.weight', 'esm.encoder.layer.2.LayerNorm.bias', 'esm.encoder.layer.24.attention.output.dense.bias', 'esm.encoder.layer.14.attention.self.key.weight', 'esm.encoder.layer.27.attention.self.query.weight', 'esm.encoder.layer.31.attention.output.dense.bias', 'esm.encoder.layer.31.attention.LayerNorm.bias', 'esm.encoder.layer.20.attention.self.value.bias', 'esm.encoder.layer.22.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.22.intermediate.dense.weight', 'esm.encoder.layer.10.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.4.attention.self.query.bias', 'esm.encoder.layer.6.attention.LayerNorm.weight', 'esm.encoder.layer.12.attention.self.query.weight', 'esm.encoder.layer.24.attention.self.query.bias', 'esm.encoder.layer.14.attention.output.dense.weight', 'esm.encoder.layer.17.output.dense.weight', 'esm.encoder.layer.21.intermediate.dense.bias', 'esm.encoder.layer.1.attention.LayerNorm.weight', 'esm.encoder.layer.15.LayerNorm.bias', 'esm.encoder.layer.1.attention.LayerNorm.bias', 'esm.encoder.layer.26.attention.self.value.bias', 'esm.encoder.layer.26.LayerNorm.bias', 'esm.encoder.layer.32.attention.LayerNorm.weight', 'esm.encoder.layer.11.attention.output.dense.bias', 'esm.encoder.layer.24.output.dense.weight', 'esm.encoder.layer.30.attention.LayerNorm.weight', 'esm.encoder.layer.10.attention.self.query.weight', 'esm.encoder.layer.4.attention.self.value.weight', 'esm.encoder.layer.19.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.1.intermediate.dense.weight', 'esm.encoder.layer.2.attention.self.value.bias', 'esm.encoder.layer.32.output.dense.bias', 'esm.encoder.layer.10.attention.self.key.weight', 'esm.encoder.layer.11.output.dense.weight', 'esm.encoder.layer.32.attention.self.query.weight', 'esm.encoder.layer.8.attention.self.query.weight', 'esm.encoder.layer.24.LayerNorm.weight', 'esm.encoder.layer.15.intermediate.dense.bias', 'esm.encoder.layer.3.output.dense.bias', 'esm.encoder.layer.18.intermediate.dense.weight', 'esm.encoder.layer.20.LayerNorm.bias', 'esm.encoder.layer.1.attention.output.dense.weight', 'esm.encoder.layer.9.output.dense.weight', 'esm.encoder.layer.17.attention.self.value.weight', 'esm.encoder.layer.32.attention.self.key.bias', 'esm.encoder.layer.26.attention.self.query.bias', 'esm.encoder.layer.17.attention.output.dense.bias', 'esm.encoder.layer.12.intermediate.dense.weight', 'esm.encoder.layer.18.attention.LayerNorm.weight', 'esm.encoder.layer.28.output.dense.bias', 'esm.encoder.layer.22.attention.self.key.bias', 'esm.encoder.layer.27.intermediate.dense.bias', 'esm.encoder.layer.13.attention.LayerNorm.weight', 'esm.encoder.layer.30.attention.LayerNorm.bias', 'esm.encoder.layer.17.attention.self.key.weight', 'esm.encoder.layer.18.intermediate.dense.bias', 'esm.encoder.layer.23.LayerNorm.bias', 'esm.encoder.layer.19.attention.self.query.weight', 'esm.encoder.layer.7.output.dense.weight', 'esm.encoder.layer.13.attention.self.value.bias', 'esm.encoder.layer.19.attention.self.key.weight', 'esm.encoder.layer.10.attention.LayerNorm.bias', 'esm.encoder.layer.10.output.dense.weight', 'esm.encoder.layer.27.attention.self.key.bias', 'esm.encoder.layer.25.LayerNorm.weight', 'esm.encoder.layer.4.attention.LayerNorm.bias', 'esm.encoder.layer.6.LayerNorm.bias', 'esm.encoder.layer.11.attention.output.dense.weight', 'esm.encoder.layer.23.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.7.attention.self.value.weight', 'esm.encoder.layer.0.intermediate.dense.bias', 'esm.encoder.layer.31.attention.output.dense.weight', 'esm.encoder.layer.29.attention.self.query.weight', 'esm.encoder.layer.16.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.21.attention.self.value.bias', 'esm.encoder.layer.11.intermediate.dense.bias', 'esm.encoder.layer.20.output.dense.weight', 'esm.encoder.layer.3.attention.LayerNorm.bias', 'esm.encoder.layer.4.attention.output.dense.weight', 'esm.encoder.layer.15.attention.self.query.weight', 'esm.encoder.layer.28.LayerNorm.weight', 'esm.encoder.layer.3.output.dense.weight', 'esm.encoder.layer.16.attention.self.value.bias', 'esm.encoder.layer.9.attention.self.value.weight', 'esm.encoder.layer.9.LayerNorm.weight', 'esm.encoder.layer.7.output.dense.bias', 'esm.encoder.layer.13.attention.output.dense.weight', 'esm.embeddings.position_ids', 'esm.encoder.layer.5.attention.output.dense.weight', 'esm.encoder.layer.4.attention.LayerNorm.weight', 'esm.encoder.layer.30.intermediate.dense.bias', 'esm.encoder.layer.9.attention.self.query.weight', 'esm.contact_head.regression.weight', 'esm.encoder.layer.5.output.dense.weight', 'esm.encoder.layer.1.LayerNorm.weight', 'esm.encoder.layer.9.attention.output.dense.bias', 'esm.encoder.layer.25.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.0.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.6.intermediate.dense.weight', 'esm.encoder.layer.30.output.dense.bias', 'esm.encoder.layer.9.attention.self.key.bias', 'esm.encoder.layer.25.attention.LayerNorm.weight', 'esm.encoder.layer.16.attention.self.query.weight', 'lm_head.bias', 'esm.encoder.layer.23.attention.self.value.bias', 'esm.encoder.layer.3.attention.LayerNorm.weight', 'esm.contact_head.regression.bias', 'esm.encoder.layer.24.attention.output.dense.weight', 'esm.encoder.layer.10.attention.self.key.bias', 'esm.encoder.layer.15.output.dense.bias', 'lm_head.layer_norm.bias', 'esm.encoder.layer.12.LayerNorm.bias', 'esm.encoder.layer.29.attention.self.value.weight', 'esm.encoder.layer.17.attention.output.dense.weight', 'esm.encoder.layer.18.attention.self.query.weight', 'esm.encoder.layer.0.LayerNorm.weight', 'esm.encoder.layer.13.attention.self.key.weight', 'esm.encoder.layer.14.LayerNorm.bias', 'esm.encoder.layer.19.attention.self.value.bias', 'esm.encoder.layer.27.attention.self.query.bias', 'esm.encoder.layer.26.attention.LayerNorm.bias', 'esm.encoder.layer.3.attention.self.key.bias', 'esm.encoder.layer.21.attention.output.dense.bias', 'esm.encoder.layer.28.attention.self.query.bias', 'esm.encoder.layer.20.LayerNorm.weight', 'esm.encoder.layer.2.attention.output.dense.bias', 'esm.encoder.layer.25.attention.self.value.weight', 'esm.encoder.layer.29.attention.LayerNorm.weight', 'esm.encoder.layer.8.attention.self.value.weight', 'esm.encoder.layer.6.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.1.output.dense.bias', 'esm.encoder.layer.5.attention.self.value.weight', 'esm.encoder.layer.11.attention.LayerNorm.bias', 'esm.encoder.layer.10.attention.self.value.bias', 'esm.encoder.layer.6.attention.self.key.bias', 'esm.encoder.layer.28.attention.self.key.bias', 'esm.encoder.layer.4.attention.output.dense.bias', 'esm.encoder.layer.29.LayerNorm.weight', 'esm.encoder.layer.29.attention.self.key.bias', 'esm.encoder.layer.24.attention.LayerNorm.bias', 'esm.encoder.layer.5.attention.LayerNorm.weight', 'esm.encoder.layer.23.attention.output.dense.weight', 'esm.encoder.layer.14.attention.self.value.weight', 'esm.encoder.layer.30.attention.output.dense.weight', 'esm.encoder.layer.19.output.dense.weight', 'esm.encoder.layer.9.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.3.LayerNorm.weight', 'esm.encoder.layer.26.attention.self.key.bias', 'esm.encoder.layer.14.intermediate.dense.weight', 'esm.encoder.layer.11.attention.self.query.bias', 'esm.encoder.layer.32.attention.LayerNorm.bias', 'esm.encoder.layer.8.attention.LayerNorm.bias', 'lm_head.dense.weight', 'esm.encoder.layer.31.attention.self.key.weight', 'esm.encoder.layer.12.attention.self.key.weight', 'esm.encoder.layer.13.attention.self.query.bias', 'esm.encoder.layer.4.output.dense.weight', 'esm.encoder.layer.15.attention.LayerNorm.bias', 'esm.encoder.layer.1.attention.self.query.weight', 'esm.encoder.layer.14.attention.LayerNorm.bias', 'esm.encoder.layer.14.attention.self.query.bias', 'esm.encoder.layer.22.attention.self.key.weight', 'esm.encoder.layer.26.attention.output.dense.bias', 'esm.encoder.layer.3.attention.self.value.bias', 'esm.encoder.layer.10.attention.output.dense.bias', 'esm.encoder.layer.1.output.dense.weight', 'esm.encoder.layer.18.output.dense.weight', 'esm.encoder.layer.16.attention.self.key.weight', 'esm.encoder.layer.30.attention.self.value.weight', 'esm.encoder.layer.11.attention.self.value.weight', 'esm.encoder.layer.27.LayerNorm.weight', 'esm.encoder.layer.18.attention.self.value.bias', 'esm.encoder.layer.23.attention.self.query.bias', 'esm.encoder.layer.6.attention.self.value.bias', 'esm.encoder.layer.9.attention.LayerNorm.weight', 'esm.encoder.layer.0.attention.self.value.weight', 'esm.encoder.layer.11.attention.LayerNorm.weight', 'esm.encoder.layer.5.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.11.LayerNorm.bias', 'esm.encoder.layer.16.output.dense.weight', 'esm.encoder.layer.16.output.dense.bias', 'esm.encoder.layer.28.output.dense.weight', 'esm.encoder.layer.5.attention.self.query.bias', 'esm.encoder.layer.15.attention.LayerNorm.weight', 'esm.encoder.layer.13.intermediate.dense.weight', 'esm.encoder.layer.14.attention.LayerNorm.weight', 'esm.encoder.layer.31.attention.self.value.weight', 'esm.encoder.layer.26.intermediate.dense.bias', 'esm.encoder.layer.30.attention.self.value.bias', 'esm.encoder.layer.30.attention.self.key.weight', 'esm.encoder.layer.8.attention.self.key.bias', 'esm.encoder.layer.19.attention.self.key.bias', 'esm.encoder.layer.9.output.dense.bias', 'esm.encoder.layer.28.LayerNorm.bias', 'esm.encoder.layer.5.LayerNorm.weight', 'esm.encoder.layer.13.attention.LayerNorm.bias', 'esm.encoder.layer.28.attention.self.value.weight', 'esm.encoder.layer.20.attention.self.query.weight', 'esm.encoder.layer.11.attention.self.key.weight', 'esm.encoder.layer.30.attention.self.query.bias', 'esm.encoder.layer.14.attention.output.dense.bias', 'esm.encoder.layer.15.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.28.attention.self.query.weight', 'esm.encoder.layer.14.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.27.attention.LayerNorm.weight', 'esm.encoder.layer.27.attention.output.dense.bias', 'esm.encoder.layer.22.LayerNorm.bias', 'esm.encoder.layer.20.attention.LayerNorm.weight', 'lm_head.dense.bias', 'esm.encoder.layer.18.LayerNorm.weight', 'esm.encoder.layer.24.attention.self.value.weight', 'esm.encoder.layer.21.output.dense.bias', 'esm.encoder.layer.18.attention.self.query.bias', 'esm.encoder.layer.9.LayerNorm.bias', 'esm.encoder.layer.23.attention.self.value.weight', 'esm.encoder.layer.27.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.25.output.dense.bias', 'esm.encoder.layer.31.attention.self.value.bias', 'esm.encoder.layer.19.output.dense.bias', 'esm.encoder.layer.2.attention.output.dense.weight', 'esm.encoder.layer.29.attention.output.dense.weight', 'esm.encoder.layer.6.attention.self.value.weight', 'esm.encoder.layer.8.intermediate.dense.weight', 'esm.encoder.layer.11.attention.self.value.bias', 'esm.encoder.layer.30.LayerNorm.bias', 'esm.encoder.layer.9.attention.self.query.bias', 'esm.encoder.layer.24.attention.self.key.bias', 'esm.encoder.layer.1.attention.self.key.weight', 'esm.encoder.layer.32.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.6.attention.self.key.weight', 'esm.encoder.layer.25.attention.self.key.weight', 'esm.encoder.layer.26.attention.self.value.weight', 'esm.encoder.layer.26.output.dense.weight', 'esm.encoder.layer.22.attention.output.dense.bias', 'esm.encoder.layer.5.output.dense.bias', 'esm.encoder.layer.28.attention.self.key.weight', 'esm.encoder.layer.1.attention.self.key.bias', 'esm.encoder.layer.7.intermediate.dense.weight', 'esm.encoder.layer.3.intermediate.dense.bias', 'esm.encoder.layer.17.LayerNorm.bias', 'esm.encoder.layer.18.output.dense.bias', 'esm.encoder.layer.26.attention.self.query.weight', 'esm.encoder.layer.6.attention.LayerNorm.bias', 'esm.encoder.layer.10.attention.output.dense.weight', 'esm.encoder.layer.2.attention.self.key.bias', 'esm.encoder.layer.21.attention.self.key.bias', 'esm.encoder.layer.3.LayerNorm.bias', 'esm.encoder.layer.18.attention.output.dense.bias', 'esm.encoder.layer.4.intermediate.dense.weight', 'esm.encoder.layer.4.attention.self.key.bias', 'esm.encoder.emb_layer_norm_after.weight', 'esm.encoder.layer.24.output.dense.bias', 'esm.encoder.layer.15.attention.self.value.weight', 'esm.encoder.layer.20.attention.output.dense.bias', 'esm.encoder.layer.25.intermediate.dense.bias', 'esm.encoder.layer.13.attention.self.key.bias', 'esm.encoder.layer.18.attention.self.value.weight', 'esm.encoder.emb_layer_norm_after.bias', 'esm.encoder.layer.16.LayerNorm.weight', 'esm.encoder.layer.29.attention.self.value.bias', 'esm.encoder.layer.13.LayerNorm.bias', 'esm.encoder.layer.28.attention.output.dense.weight', 'esm.encoder.layer.21.attention.LayerNorm.bias', 'esm.encoder.layer.18.attention.LayerNorm.bias', 'esm.encoder.layer.17.attention.LayerNorm.bias', 'esm.encoder.layer.12.intermediate.dense.bias', 'esm.encoder.layer.14.attention.self.key.bias', 'esm.encoder.layer.29.intermediate.dense.weight', 'esm.encoder.layer.23.attention.output.dense.bias', 'esm.encoder.layer.3.attention.output.dense.weight', 'esm.encoder.layer.5.attention.self.value.bias', 'esm.encoder.layer.24.attention.self.value.bias', 'esm.encoder.layer.12.attention.self.query.bias', 'esm.encoder.layer.11.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.29.attention.self.key.weight', 'esm.encoder.layer.7.attention.self.key.weight', 'esm.encoder.layer.5.intermediate.dense.bias', 'esm.encoder.layer.21.output.dense.weight', 'esm.encoder.layer.27.attention.LayerNorm.bias', 'esm.encoder.layer.0.output.dense.bias', 'esm.encoder.layer.19.attention.LayerNorm.bias', 'esm.encoder.layer.0.attention.self.key.weight', 'esm.encoder.layer.5.attention.output.dense.bias', 'esm.encoder.layer.16.attention.self.query.bias', 'esm.encoder.layer.29.output.dense.bias', 'esm.encoder.layer.2.attention.self.query.bias', 'esm.encoder.layer.13.attention.self.query.weight', 'esm.encoder.layer.12.attention.self.key.bias', 'esm.encoder.layer.4.attention.self.query.weight', 'esm.encoder.layer.0.LayerNorm.bias', 'esm.encoder.layer.19.attention.output.dense.weight', 'esm.encoder.layer.22.attention.LayerNorm.bias', 'esm.encoder.layer.22.attention.self.value.weight', 'esm.encoder.layer.14.output.dense.bias', 'esm.encoder.layer.22.attention.self.value.bias', 'esm.encoder.layer.20.attention.self.query.bias', 'esm.encoder.layer.21.attention.self.query.weight', 'esm.encoder.layer.23.attention.LayerNorm.weight', 'esm.encoder.layer.32.LayerNorm.weight', 'lm_head.layer_norm.weight', 'esm.encoder.layer.8.attention.output.dense.bias', 'esm.encoder.layer.15.attention.output.dense.weight', 'esm.encoder.layer.5.LayerNorm.bias', 'esm.encoder.layer.22.output.dense.bias', 'esm.encoder.layer.5.attention.LayerNorm.bias', 'esm.encoder.layer.17.output.dense.bias', 'esm.encoder.layer.2.attention.self.query.weight', 'esm.encoder.layer.27.attention.self.value.weight', 'esm.encoder.layer.30.attention.self.key.bias', 'esm.encoder.layer.25.attention.LayerNorm.bias', 'esm.encoder.layer.7.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.11.intermediate.dense.weight', 'esm.encoder.layer.1.attention.self.value.bias', 'esm.encoder.layer.0.attention.self.key.bias', 'esm.encoder.layer.12.attention.output.dense.bias', 'esm.encoder.layer.31.output.dense.weight', 'esm.encoder.layer.0.attention.LayerNorm.weight', 'esm.encoder.layer.11.LayerNorm.weight', 'esm.encoder.layer.0.attention.output.dense.bias', 'esm.encoder.layer.20.intermediate.dense.bias', 'esm.encoder.layer.22.attention.self.query.bias', 'esm.encoder.layer.16.intermediate.dense.weight', 'esm.encoder.layer.2.attention.LayerNorm.weight', 'esm.encoder.layer.19.attention.output.dense.bias', 'esm.encoder.layer.8.output.dense.weight', 'esm.encoder.layer.17.intermediate.dense.weight', 'esm.encoder.layer.2.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.11.output.dense.bias', 'esm.encoder.layer.27.output.dense.weight', 'esm.encoder.layer.19.attention.self.value.weight', 'esm.encoder.layer.21.attention.self.query.bias', 'esm.encoder.layer.30.intermediate.dense.weight', 'esm.encoder.layer.10.output.dense.bias', 'esm.encoder.layer.23.attention.self.query.weight', 'esm.encoder.layer.30.output.dense.weight', 'esm.encoder.layer.31.attention.LayerNorm.weight', 'esm.encoder.layer.9.attention.self.key.weight', 'esm.encoder.layer.29.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.12.output.dense.bias', 'esm.encoder.layer.6.attention.self.query.weight', 'esm.encoder.layer.20.attention.self.rotary_embeddings.inv_freq', 'esm.encoder.layer.2.output.dense.bias', 'esm.encoder.layer.1.LayerNorm.bias', 'esm.encoder.layer.31.attention.self.query.bias', 'esm.encoder.layer.20.output.dense.bias', 'esm.encoder.layer.17.attention.self.value.bias', 'esm.encoder.layer.15.intermediate.dense.weight', 'esm.encoder.layer.2.attention.self.key.weight', 'esm.encoder.layer.8.attention.LayerNorm.weight', 'esm.encoder.layer.32.intermediate.dense.weight', 'esm.encoder.layer.13.output.dense.weight', 'esm.encoder.layer.9.intermediate.dense.weight', 'esm.encoder.layer.31.LayerNorm.bias', 'esm.encoder.layer.14.output.dense.weight', 'esm.encoder.layer.26.attention.self.key.weight']\n",
      "- This IS expected if you are initializing BertRnn from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertRnn from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertRnn were not initialized from the model checkpoint at /M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/ and are newly initialized: ['encoder.layer.0.attention.output.dense.bias', 'encoder.layer.27.attention.self.key.weight', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.26.attention.self.value.weight', 'encoder.layer.29.attention.output.LayerNorm.bias', 'encoder.layer.14.attention.self.key.weight', 'rnn.weight_hh_l1', 'encoder.layer.32.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.21.output.dense.weight', 'rnn.bias_hh_l1_reverse', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.28.output.LayerNorm.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.22.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.31.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.29.output.dense.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.25.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.26.output.LayerNorm.weight', 'encoder.layer.19.output.LayerNorm.bias', 'rnn.weight_ih_l0', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.30.attention.self.query.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.14.attention.self.value.weight', 'rnn.bias_ih_l1', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.26.attention.self.value.bias', 'encoder.layer.30.attention.self.query.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.24.attention.self.value.weight', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.32.output.LayerNorm.weight', 'encoder.layer.25.attention.self.key.bias', 'rnn.bias_hh_l0', 'rnn.weight_hh_l0', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.17.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'rnn.weight_ih_l1_reverse', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.26.intermediate.dense.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.24.output.dense.weight', 'encoder.layer.8.attention.self.value.bias', 'pooler.dense.weight', 'encoder.layer.25.attention.self.value.bias', 'encoder.layer.24.attention.self.key.weight', 'encoder.layer.24.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'classifier.bias', 'encoder.layer.28.attention.self.query.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.30.output.LayerNorm.bias', 'encoder.layer.13.attention.self.key.weight', 'pooler.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'classifier.weight', 'encoder.layer.28.output.LayerNorm.weight', 'encoder.layer.28.output.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.27.attention.self.query.weight', 'encoder.layer.30.attention.self.key.weight', 'encoder.layer.28.attention.output.LayerNorm.weight', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.31.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.18.output.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.31.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'rnn.weight_ih_l0_reverse', 'encoder.layer.24.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.32.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.32.attention.self.value.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.28.attention.self.value.weight', 'encoder.layer.31.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.26.output.dense.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.24.attention.self.query.weight', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.31.attention.self.key.bias', 'rnn.weight_ih_l1', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.29.output.dense.bias', 'encoder.layer.24.output.LayerNorm.weight', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.26.output.LayerNorm.bias', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.30.output.LayerNorm.weight', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.27.attention.self.key.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.26.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.24.attention.self.value.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.24.attention.self.query.bias', 'encoder.layer.14.output.dense.bias', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.26.attention.self.query.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.25.attention.self.query.bias', 'encoder.layer.19.output.dense.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.23.output.dense.weight', 'encoder.layer.30.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.27.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.30.intermediate.dense.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.31.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.26.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.22.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.26.output.dense.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.30.output.dense.bias', 'encoder.layer.29.attention.self.key.weight', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.24.output.LayerNorm.bias', 'encoder.layer.32.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.29.attention.self.query.bias', 'encoder.layer.31.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.29.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.24.attention.self.key.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.32.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.27.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.32.output.LayerNorm.bias', 'encoder.layer.25.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.value.bias', 'rnn.weight_hh_l1_reverse', 'encoder.layer.25.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.19.attention.output.dense.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.32.attention.output.dense.bias', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.28.attention.output.LayerNorm.bias', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.28.intermediate.dense.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.28.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.30.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.27.output.LayerNorm.bias', 'encoder.layer.28.attention.self.key.bias', 'encoder.layer.27.output.dense.weight', 'encoder.layer.31.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.24.attention.output.LayerNorm.weight', 'encoder.layer.26.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.25.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.28.attention.self.value.bias', 'encoder.layer.28.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.27.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.20.intermediate.dense.weight', 'rnn.bias_ih_l0_reverse', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.13.output.dense.weight', 'encoder.layer.29.attention.self.key.bias', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.24.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.29.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.27.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.25.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.26.attention.self.query.weight', 'encoder.layer.21.attention.self.key.bias', 'encoder.layer.29.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.32.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.30.attention.output.LayerNorm.weight', 'encoder.layer.13.output.dense.bias', 'encoder.layer.28.attention.self.key.weight', 'rnn.bias_hh_l0_reverse', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.30.attention.self.value.weight', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.25.attention.self.value.weight', 'encoder.layer.32.output.dense.weight', 'encoder.layer.31.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.29.attention.self.value.bias', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.28.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.27.intermediate.dense.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.32.output.dense.bias', 'encoder.layer.25.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.30.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.24.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.25.output.dense.weight', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.25.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.15.intermediate.dense.weight', 'rnn.bias_hh_l1', 'rnn.bias_ih_l1_reverse', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.25.intermediate.dense.weight', 'rnn.weight_hh_l0_reverse', 'encoder.layer.25.attention.self.query.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.19.output.dense.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.27.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.31.attention.self.key.weight', 'encoder.layer.26.attention.output.LayerNorm.bias', 'encoder.layer.30.attention.output.LayerNorm.bias', 'encoder.layer.32.attention.self.key.bias', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.18.attention.output.dense.weight', 'rnn.bias_ih_l0', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.27.attention.self.value.weight', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.23.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.29.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.27.attention.self.query.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.26.attention.self.key.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.30.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.32.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.29.intermediate.dense.bias', 'encoder.layer.24.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.32.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.29.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.20.output.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.22.output.LayerNorm.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.27.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.29.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.31.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.31.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.31.attention.self.query.bias', 'encoder.layer.32.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.31.attention.output.LayerNorm.weight', 'encoder.layer.12.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/vicente/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 256,514 || all params: 679,378,692 || trainable%: 0.03775714531829915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-201000.\n",
      "***** Running training *****\n",
      "  Num examples = 539019\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 202134\n",
      "  Number of trainable parameters = 256514\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 5\n",
      "  Continuing training from global step 201000\n",
      "  Will skip the first 5 epochs then the first 32555 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
      "Skipping the first batches:   0%|          | 0/32555 [00:00<?, ?it/s]Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvmachacaa\u001b[0m (\u001b[33mbioargos\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vicente/projects/argosMHC/wandb/run-20240619_082058-ypxchc1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bioargos/huggingface/runs/ypxchc1x' target=\"_blank\">/M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/</a></strong> to <a href='https://wandb.ai/bioargos/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bioargos/huggingface' target=\"_blank\">https://wandb.ai/bioargos/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bioargos/huggingface/runs/ypxchc1x' target=\"_blank\">https://wandb.ai/bioargos/huggingface/runs/ypxchc1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping the first batches: 100%|| 32555/32555 [01:09<00:00, 470.04it/s]\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-195000/ (score: 0.7632602449117809).\n",
      "\n",
      "100%|| 202134/202134 [07:02<00:00, 478.07it/s]\n",
      "Saving model checkpoint to /M2/ArgosMHC_models/models/lora_t33_c3/\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 427.4167, 'train_samples_per_second': 7566.653, 'train_steps_per_second': 472.92, 'train_loss': 0.002940129368949713, 'epoch': 6.0}\n",
      "finish training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /M2/ArgosMHC_models/models/lora_t33_c3/config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertConfig, AdamW\n",
    "from model_utils_bert import BertLinear, BertRnn, BertRnnAtt, BertRnnSigmoid\n",
    "from model_utils_tape import TapeLinear, TapeRnn, TapeRnnAtt, TapeRnnDist\n",
    "from utils import compute_metrics\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy\n",
    "\n",
    "from tape import ProteinBertConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler, TrainerCallback\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import numpy as np\n",
    "import os\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "# data loaders\n",
    "from dataloader_bert import DataSetLoaderBERT, DataSetLoaderBERT_old\n",
    "from dataloader_tape import DataSetLoaderTAPE\n",
    "\n",
    "import wandb\n",
    "from transformers import set_seed\n",
    "set_seed(10)\n",
    "#set_seed(1)\n",
    "\n",
    "path_checkpoints    = \"/M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/\"  # path to store checkpoints\n",
    "path_model          = \"/M2/ArgosMHC_models/models/lora_t33_c3/\"       # path to save the best model\n",
    "model_name          = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"   # path of the pre-trained model, for esm2 and protbert\n",
    "\n",
    "path_train_csv = \"/M2/ArgosMHC_models/dataset/hlab/hlab_train.csv\"\n",
    "path_val_csv = \"/M2/ArgosMHC_models/dataset/hlab/hlab_val.csv\"\n",
    "\n",
    "max_length = 50 # for hlab dataset\n",
    "\n",
    "trainset = DataSetLoaderBERT(path=path_train_csv, tokenizer_name=model_name, max_length=max_length)\n",
    "valset = DataSetLoaderBERT(path=path_val_csv, tokenizer_name=model_name, max_length=max_length)    \n",
    "config = BertConfig.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "config.rnn = \"lstm\"\n",
    "config.num_rnn_layer = 2\n",
    "config.rnn_dropout = 0.1\n",
    "config.rnn_hidden = 768\n",
    "config.length = max_length\n",
    "config.cnn_filters = 512\n",
    "config.cnn_dropout = 0.1\n",
    "                     \n",
    "model_ = BertRnn.from_pretrained(model_name, config=config)\n",
    "\n",
    "############ hyperparameters #################################################### Configuration 3\n",
    "num_samples = len(trainset)\n",
    "num_epochs = 6\n",
    "batch_size = 16  \n",
    "weight_decay = 0.01\n",
    "lr =2e-5\n",
    "betas = ((0.9, 0.98)) \n",
    "num_training_steps = int((num_epochs * num_samples)/batch_size) \n",
    "warmup_steps = int(num_training_steps*0.1)\n",
    "\n",
    "# LoRA config ####################################################################\n",
    "configLora = { \"lora_alpha\": 1, \"lora_dropout\": 0.4, \"r\": 1 }\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS, \n",
    "    inference_mode=False, \n",
    "    r=configLora[\"r\"], \n",
    "    lora_alpha=configLora[\"lora_alpha\"], \n",
    "    target_modules=[\"query\", \"key\", \"value\"], # also maybe \"dense_h_to_4h\" and \"dense_4h_to_h\"\n",
    "    lora_dropout=configLora[\"lora_dropout\"], \n",
    "    bias=\"none\" # or \"all\" or \"lora_only\" \n",
    ")\n",
    "\n",
    "model_ = get_peft_model(model_, peft_config)\n",
    "#model_ = accelerator.prepare(model_)\n",
    "model_.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir                  = path_checkpoints, \n",
    "        num_train_epochs            = num_epochs,   \n",
    "        per_device_train_batch_size = batch_size,   \n",
    "        per_device_eval_batch_size  = batch_size * 8,         \n",
    "        logging_dir                 = path_checkpoints,        \n",
    "        logging_strategy            = \"steps\", #epoch or steps\n",
    "        eval_steps                  = 3000, # el primer experimento fue con 1000 steps\n",
    "        save_steps                  = 3000,\n",
    "        metric_for_best_model       = 'f1',\n",
    "        load_best_model_at_end      = True,        \n",
    "        evaluation_strategy         = \"steps\", #epoch or steps\n",
    "        save_strategy               = \"steps\", #epoch or ste  \n",
    "        logging_steps=3000  # how often to log to W&B\n",
    ")\n",
    "\n",
    "optimizer = AdamW(model_.parameters(), lr=lr, betas=betas, weight_decay=weight_decay, correct_bias=True)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "trainer = Trainer(        \n",
    "        args            = training_args,   \n",
    "        model           = model_, \n",
    "        train_dataset   = trainset,  \n",
    "        eval_dataset    = valset, \n",
    "        compute_metrics = compute_metrics,  \n",
    "        optimizers      = (optimizer, lr_scheduler),      \n",
    "        callbacks       = [EarlyStoppingCallback(early_stopping_patience=5)] \n",
    "    )\n",
    "\n",
    "trainer.train(resume_from_checkpoint = True)\n",
    "print(\"finish training\")\n",
    "trainer.save_model(path_model)\n",
    "trainer.model.config.save_pretrained(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82285b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForTokenClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertRnn(\n",
      "      (bert): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "          (position_embeddings): Embedding(1026, 1280)\n",
      "          (token_type_embeddings): Embedding(2, 1280)\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (12): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (13): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (14): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (15): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (16): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (17): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (18): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (19): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (20): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (21): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (22): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (23): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (24): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (25): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (26): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (27): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (28): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (29): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (30): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (31): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (32): BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.4, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1280, out_features=1, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=1, out_features=1280, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (rnn): LSTM(1280, 768, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=1536, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=1536, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (criterion): CrossEntropyLoss()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea26e1a",
   "metadata": {},
   "source": [
    "When we opends the model using Trainer, the models inference well. So, we are missing a step when oopen the model. Moreover, when we print the model, we noticed that it is a PeftModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c25ba9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "***** Running Prediction *****\n",
      "  Num examples = 76\n",
      "  Batch size = 8\n",
      "100%|| 10/10 [00:00<00:00, 14.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0] [[-0.72885877  0.7541259 ]\n",
      " [ 0.68282026 -0.63367534]\n",
      " [-1.0395614   0.9736944 ]\n",
      " [-1.0817848   1.0852101 ]\n",
      " [-0.46247494  0.55944276]\n",
      " [-0.56534374  0.5921434 ]\n",
      " [-0.83514065  0.8107847 ]\n",
      " [-1.2630697   1.1586009 ]\n",
      " [-0.95792246  0.980892  ]\n",
      " [-1.0324655   0.9924274 ]\n",
      " [-0.76731664  0.7217846 ]\n",
      " [-1.6330781   1.514336  ]\n",
      " [-0.7957608   0.8229094 ]\n",
      " [-0.9124353   0.8701133 ]\n",
      " [-1.2138525   1.209451  ]\n",
      " [-1.0927274   1.0711197 ]\n",
      " [-1.0666986   1.0522016 ]\n",
      " [-0.45787728  0.42578974]\n",
      " [-0.8832142   0.76278913]\n",
      " [-0.37021875  0.34802938]\n",
      " [-1.2862854   1.2059811 ]\n",
      " [-0.8621023   0.8841515 ]\n",
      " [-0.7213802   0.6836574 ]\n",
      " [-0.4424507   0.43031347]\n",
      " [-0.7615446   0.83321893]\n",
      " [-1.278306    1.2741243 ]\n",
      " [-0.90015024  0.880399  ]\n",
      " [-0.39320016  0.33496857]\n",
      " [ 0.24058262 -0.17222066]\n",
      " [-1.2310044   1.2370495 ]\n",
      " [-0.3303253   0.29723084]\n",
      " [-1.1192397   1.1065507 ]\n",
      " [-1.3087485   1.2718021 ]\n",
      " [-0.69259995  0.63280797]\n",
      " [-0.43699306  0.4794941 ]\n",
      " [-0.61308205  0.5710803 ]\n",
      " [-0.35209945  0.3534966 ]\n",
      " [-1.1015838   1.0338595 ]\n",
      " [ 0.02459919  0.00284942]\n",
      " [ 1.0113033  -1.0051184 ]\n",
      " [ 0.15725964 -0.15114497]\n",
      " [-0.18845685  0.23676075]\n",
      " [-0.09651806  0.20366228]\n",
      " [ 0.7251431  -0.74793625]\n",
      " [ 0.16148332 -0.22538112]\n",
      " [ 0.6537901  -0.61034966]\n",
      " [-0.47891167  0.46314844]\n",
      " [ 0.28770584 -0.2712823 ]\n",
      " [-0.04469442  0.08074659]\n",
      " [ 1.09431    -1.0502859 ]\n",
      " [ 0.13188209 -0.03953332]\n",
      " [ 1.125375   -1.1552272 ]\n",
      " [-0.36760455  0.3883926 ]\n",
      " [ 0.8173655  -0.7502447 ]\n",
      " [-0.27115166  0.33281946]\n",
      " [-0.59391415  0.5968375 ]\n",
      " [ 1.5522612  -1.5394182 ]\n",
      " [ 0.06386697 -0.05078483]\n",
      " [ 0.9083721  -0.83864075]\n",
      " [ 0.84519994 -0.8528756 ]\n",
      " [ 0.8207889  -0.7343579 ]\n",
      " [ 0.10903409 -0.26296216]\n",
      " [-0.46873918  0.3938081 ]\n",
      " [-0.17431048  0.30912638]\n",
      " [-0.13708876  0.14139308]\n",
      " [-0.35004836  0.4320581 ]\n",
      " [-0.29734042  0.30535877]\n",
      " [ 0.98818976 -0.9636773 ]\n",
      " [ 0.876019   -0.8166454 ]\n",
      " [ 0.96252936 -0.9128873 ]\n",
      " [ 1.4046506  -1.505768  ]\n",
      " [-0.11211377  0.17874236]\n",
      " [ 0.76360184 -0.7864359 ]\n",
      " [ 1.0391519  -0.9843213 ]\n",
      " [ 0.01263529  0.01738924]\n",
      " [ 0.13732338 -0.06335512]]\n",
      "           0         1  prediction  label\n",
      "0  -0.728859  0.754126           1      1\n",
      "1   0.682820 -0.633675           0      1\n",
      "2  -1.039561  0.973694           1      1\n",
      "3  -1.081785  1.085210           1      1\n",
      "4  -0.462475  0.559443           1      1\n",
      "..       ...       ...         ...    ...\n",
      "71 -0.112114  0.178742           1      0\n",
      "72  0.763602 -0.786436           0      0\n",
      "73  1.039152 -0.984321           0      0\n",
      "74  0.012635  0.017389           1      0\n",
      "75  0.137323 -0.063355           0      0\n",
      "\n",
      "[76 rows x 4 columns]\n",
      "{'test_loss': 0.40289023518562317, 'test_auc': 0.7894736842105262, 'test_precision': 0.72, 'test_recall': 0.9473684210526315, 'test_f1': 0.8181818181818181, 'test_sn': 0.72, 'test_sp': 0.631578947368421, 'test_accuracy': 0.7894736842105263, 'test_mcc': 0.610170215847752, 'test_runtime': 0.761, 'test_samples_per_second': 99.869, 'test_steps_per_second': 13.141}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t6_8M_UR50D/\"\n",
    "dataset = \"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\"\n",
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "\n",
    "model_trainer = Trainer(model = model_, compute_metrics = compute_metrics)\n",
    "test_dataset = DataSetLoaderBERT(dataset, tokenizer_name=pre_trained, max_length=seq_length)\n",
    "predictions, label_ids, metrics = model_trainer.predict(test_dataset)\n",
    "\n",
    "df = pd.DataFrame(predictions)\n",
    "\n",
    "df['prediction'] = df.apply(lambda row: ( 0 if row[0] > row[1] else 1 ), axis=1)\n",
    "df['label'] = label_ids\n",
    "print(df)\n",
    "#df.to_csv(name_results + \".csv\")\n",
    "\n",
    "#print(predictions)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dc2af",
   "metadata": {},
   "source": [
    "# 3 Predict Classic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42390a7f",
   "metadata": {},
   "source": [
    "Este cdigo, evalua el modelo despues del fine-tuning en el conjunto de testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "edb1143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline, pipeline, BartForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import set_seed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c31c82",
   "metadata": {},
   "source": [
    "Abrimos el modelo y el tokenizer. Tambien definimos la ruta del modelo, el tokenizer (modelo pre entrenado) y la base de datos de testing. Finalmente, definimos el sufijo de de los archivos a generar como salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1672458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../pre_trained_models/esm2_t6_8M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"BertRnn\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cnn_dropout\": 0.1,\n",
      "  \"cnn_filters\": 512,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 320,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1280,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length\": 50,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.1,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n",
      "loading weights file /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertRnn.\n",
      "\n",
      "All the weights of BertRnn were initialized from the model checkpoint at /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertRnn for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/\"  # mejor checkpoiunt\n",
    "name_results = \"predictions_esm2_t33_c5\" # \n",
    "#pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t6_8M_UR50D/\"\n",
    "dataset = \"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\"\n",
    "\n",
    "model = BertRnn.from_pretrained(model_name, num_labels=2) # it fail for automodel for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbbe1f6",
   "metadata": {},
   "source": [
    "Definimos el DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b33a1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(dataset, tokenizer_name=pre_trained, max_length=seq_length)\n",
    "data_iter = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "print(type(test_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f296a47",
   "metadata": {},
   "source": [
    "Prediccion usando el dataloader y segn el batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b6978f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    logit_class_0  logit_class_1  prob_class_0  prob_class_1  prediction  \\\n",
      "0        1.484725      -2.111731      0.973311      0.026689           0   \n",
      "1        0.366563      -0.806652      0.763726      0.236274           0   \n",
      "2       -3.797246       3.086962      0.001023      0.998977           1   \n",
      "3       -4.328579       3.746734      0.000311      0.999689           1   \n",
      "4       -1.433432       0.912185      0.087415      0.912585           1   \n",
      "..            ...            ...           ...           ...         ...   \n",
      "71       1.846199      -2.293438      0.984321      0.015679           0   \n",
      "72      -0.228940      -0.585798      0.588280      0.411720           0   \n",
      "73       1.457081      -2.315464      0.977523      0.022477           0   \n",
      "74       1.226935      -1.907617      0.958296      0.041704           0   \n",
      "75       2.441628      -3.086117      0.996041      0.003959           0   \n",
      "\n",
      "    label  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "..    ...  \n",
      "71      0  \n",
      "72      0  \n",
      "73      0  \n",
      "74      0  \n",
      "75      0  \n",
      "\n",
      "[76 rows x 6 columns]\n",
      "{'accuracy': 0.881578947368421, 'precision': 0.9393939393939394, 'recall': 0.8157894736842105, 'f1score': 0.8732394366197183, 'auc': 0.9439058171745152, 'mcc': 0.7698512161427534}\n"
     ]
    }
   ],
   "source": [
    "# data_iter, es un dataLoader, de la base de datos de test, y tiene un batch_size de 16\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "model.eval() # is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation,\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad(): # turn off gradients computation\n",
    "    for i, batch in enumerate(data_iter): # por cada batch        \n",
    "        labels.extend(batch['labels'].numpy())\n",
    "        output = model(batch['input_ids'], batch['attention_mask']) # inference\n",
    "        for row_sample in output.logits: # por cada muestra del batch\n",
    "            logits = row_sample.numpy()\n",
    "            probs = softmax(logits)\n",
    "            predictions.append( [logits[0], logits[1], probs[0], probs[1]] )\n",
    "\n",
    "#print(predictions)\n",
    "df = pd.DataFrame(predictions, columns=[\"logit_class_0\", \"logit_class_1\", \"prob_class_0\", \"prob_class_1\"])\n",
    "df['prediction'] = df.apply(lambda row: ( 0 if row[0] > row[1] else 1 ), axis=1)\n",
    "df['label'] = labels\n",
    "print(df)\n",
    "\n",
    "print(get_metrics(df['label'], df['prediction'], df['prob_class_1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96837f7b",
   "metadata": {},
   "source": [
    "Ejemplo de como hacer la Prediccion usando una sola muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "937fe41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output SequenceClassifierOutput(loss=None, logits=tensor([-4.3286,  3.7467]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# algunas muestras\n",
    "\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYRSDTPLIY\" # label 1\n",
    "\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYNSDLVQKY\" # label 1\n",
    "\"YFAMYGEKVAHTHVDTLYLRYHYYTWAVWAYTWYLLAASEAPR\"  # label 0\n",
    "\"YFAMYGEKVAHTHVDTLYLRYHYYTWAVWAYTWYQWSEKVTEE\"  # label 0\n",
    "\n",
    "# el tokenizer devuelve los input_ids y el attention_mask como dos listas\n",
    "sample = tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYNSDLVQKY\", padding='max_length', max_length=seq_length)\n",
    "\n",
    "# convertimos en tensor, debe ser lista de listas\n",
    "ids = torch.IntTensor([sample['input_ids']]) # tensor 2D\n",
    "masks = torch.IntTensor([sample['attention_mask']]) # tensor 2D\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # turn off gradients computation\n",
    "    output = model(ids, masks )   \n",
    "\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7d2aa",
   "metadata": {},
   "source": [
    "# 4 Predict classic with Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a945292",
   "metadata": {},
   "source": [
    "Este ejemplo, tambien hace las predicciones y clcula las mtricas, pero usando el Trainner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0d029f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../pre_trained_models/esm2_t6_8M_UR50D\",\n",
      "  \"architectures\": [\n",
      "    \"BertRnn\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cnn_dropout\": 0.1,\n",
      "  \"cnn_filters\": 512,\n",
      "  \"emb_layer_norm_before\": false,\n",
      "  \"esmfold_config\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 320,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1280,\n",
      "  \"is_folding_model\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"length\": 50,\n",
      "  \"mask_token_id\": 32,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 20,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_rnn_layer\": 2,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"rotary\",\n",
      "  \"rnn\": \"lstm\",\n",
      "  \"rnn_dropout\": 0.1,\n",
      "  \"rnn_hidden\": 768,\n",
      "  \"token_dropout\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_list\": null,\n",
      "  \"vocab_size\": 33\n",
      "}\n",
      "\n",
      "loading weights file /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertRnn.\n",
      "\n",
      "All the weights of BertRnn were initialized from the model checkpoint at /M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertRnn for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "***** Running Prediction *****\n",
      "  Num examples = 76\n",
      "  Batch size = 8\n",
      "100%|| 10/10 [00:00<00:00, 84.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1  prediction  label\n",
      "0   1.484571 -2.111625           0      1\n",
      "1   0.366120 -0.806223           0      1\n",
      "2  -3.797319  3.086969           1      1\n",
      "3  -4.328716  3.746843           1      1\n",
      "4  -1.433371  0.912052           1      1\n",
      "..       ...       ...         ...    ...\n",
      "71  1.846014 -2.293381           0      0\n",
      "72 -0.229090 -0.585730           0      0\n",
      "73  1.457083 -2.315473           0      0\n",
      "74  1.226967 -1.907715           0      0\n",
      "75  2.441543 -3.086087           0      0\n",
      "\n",
      "[76 rows x 4 columns]\n",
      "{'test_loss': 0.3903467059135437, 'test_auc': 0.8815789473684211, 'test_precision': 0.9393939393939394, 'test_recall': 0.8157894736842105, 'test_f1': 0.8732394366197183, 'test_sn': 0.9393939393939394, 'test_sp': 0.9473684210526315, 'test_accuracy': 0.881578947368421, 'test_mcc': 0.7698512161427534, 'test_runtime': 0.1327, 'test_samples_per_second': 572.778, 'test_steps_per_second': 75.366}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions for HLAB dataset for TAPE\n",
    "\n",
    "# load model\n",
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from transformers import BertConfig\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "\n",
    "model_name = \"/M2/ArgosMHC_models/checkpoints/classic_t6_c3/checkpoint-102000/\"  # mejor checkpoiunt\n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t6_8M_UR50D/\"\n",
    "dataset = \"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\"\n",
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "config = BertConfig.from_pretrained(model_name, num_labels=2 )\n",
    "\n",
    "model = Trainer(model = BertRnn.from_pretrained(model_name, config=config), compute_metrics = compute_metrics)\n",
    "test_dataset = DataSetLoaderBERT(dataset, tokenizer_name=pre_trained, max_length=seq_length)\n",
    "predictions, label_ids, metrics = model.predict(test_dataset)\n",
    "\n",
    "df = pd.DataFrame(predictions)\n",
    "\n",
    "df['prediction'] = df.apply(lambda row: ( 0 if row[0] > row[1] else 1 ), axis=1)\n",
    "df['label'] = label_ids\n",
    "print(df)\n",
    "#df.to_csv(name_results + \".csv\")\n",
    "\n",
    "#print(predictions)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
