{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3118b43e",
   "metadata": {},
   "source": [
    "# Predict with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "325c6df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb20c02",
   "metadata": {},
   "source": [
    "Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16366545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ were not used when initializing BertForSequenceClassification: ['base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.5.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.16.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.26.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.bias', 'base_model.model.rnn.weight_hh_l0', 'base_model.model.bert.encoder.layer.0.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.lora_A.default.weight', 'base_model.model.classifier.original_module.bias', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.bias', 'base_model.model.bert.embeddings.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.4.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.21.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.output.dense.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.weight', 'base_model.model.bert.embeddings.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.21.output.dense.weight', 'base_model.model.bert.encoder.layer.14.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.bias', 'base_model.model.rnn.bias_hh_l0_reverse', 'base_model.model.bert.encoder.layer.22.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.output.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.intermediate.dense.weight', 'base_model.model.bert.embeddings.position_embeddings.weight', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_B.default.weight', 'base_model.model.rnn.weight_hh_l1', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.bias', 'base_model.model.rnn.bias_hh_l1_reverse', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.8.output.dense.bias', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.14.output.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.23.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.19.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.28.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.21.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.12.output.dense.bias', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.0.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.output.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.bias', 'base_model.model.classifier.original_module.weight', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.4.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.9.output.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.output.dense.weight', 'base_model.model.bert.encoder.layer.18.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.lora_A.default.weight', 'base_model.model.rnn.weight_hh_l1_reverse', 'base_model.model.bert.encoder.layer.6.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.22.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.23.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.weight', 'base_model.model.bert.pooler.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.5.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.output.dense.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.output.dense.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.31.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.20.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.9.output.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.dense.bias', 'base_model.model.bert.encoder.layer.24.output.dense.bias', 'base_model.model.bert.encoder.layer.17.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_ih_l0', 'base_model.model.bert.encoder.layer.4.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.14.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.26.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.19.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.output.dense.weight', 'base_model.model.bert.encoder.layer.23.output.dense.bias', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.output.LayerNorm.bias', 'base_model.model.rnn.weight_ih_l1', 'base_model.model.bert.encoder.layer.23.intermediate.dense.weight', 'base_model.model.classifier.modules_to_save.default.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.output.dense.bias', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.output.dense.weight', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.32.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.11.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.32.output.dense.bias', 'base_model.model.bert.encoder.layer.8.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.30.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.10.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.22.output.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.27.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.24.output.dense.weight', 'base_model.model.bert.encoder.layer.28.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.output.dense.bias', 'base_model.model.bert.encoder.layer.14.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.32.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.8.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.6.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.13.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.output.dense.weight', 'base_model.model.bert.encoder.layer.29.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.23.output.dense.weight', 'base_model.model.bert.encoder.layer.19.output.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.output.dense.weight', 'base_model.model.rnn.weight_ih_l0', 'base_model.model.bert.encoder.layer.24.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.key.base_layer.weight', 'base_model.model.rnn.weight_ih_l0_reverse', 'base_model.model.bert.encoder.layer.1.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.0.output.dense.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight', 'base_model.model.rnn.bias_ih_l0_reverse', 'base_model.model.bert.encoder.layer.15.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.word_embeddings.weight', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.22.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.30.output.dense.bias', 'base_model.model.bert.encoder.layer.3.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.7.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.10.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.31.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.8.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.20.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.23.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.1.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.17.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.output.dense.bias', 'base_model.model.bert.encoder.layer.12.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.16.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.1.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.16.output.dense.weight', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.31.intermediate.dense.bias', 'base_model.model.rnn.weight_hh_l0_reverse', 'base_model.model.bert.encoder.layer.19.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.0.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.9.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.11.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.9.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.1.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.12.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.22.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.32.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.4.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.15.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.11.output.dense.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.0.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.12.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.5.attention.self.value.base_layer.weight', 'base_model.model.bert.embeddings.position_ids', 'base_model.model.bert.embeddings.token_type_embeddings.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.18.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.output.dense.bias', 'base_model.model.bert.encoder.layer.26.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.12.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.15.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.5.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.intermediate.dense.weight', 'base_model.model.rnn.bias_ih_l1', 'base_model.model.bert.encoder.layer.16.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.10.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.2.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.28.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.29.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.15.output.dense.bias', 'base_model.model.bert.encoder.layer.3.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.2.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.5.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.13.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.9.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.22.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.20.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.0.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.25.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.27.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.7.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.3.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.23.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.7.output.dense.weight', 'base_model.model.bert.encoder.layer.14.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.32.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.17.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.26.output.dense.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.20.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.0.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.21.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.7.output.dense.bias', 'base_model.model.rnn.bias_ih_l1_reverse', 'base_model.model.bert.encoder.layer.32.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.32.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.32.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.0.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.25.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.13.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.21.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.30.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.27.output.dense.bias', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.3.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.16.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.13.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.1.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.2.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.17.output.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.12.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.24.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.19.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.2.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.3.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.13.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.21.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.24.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.13.attention.self.query.base_layer.weight', 'base_model.model.rnn.bias_hh_l1', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.26.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.32.output.dense.weight', 'base_model.model.bert.encoder.layer.4.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.25.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.23.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.6.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.21.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.24.attention.output.dense.weight', 'base_model.model.classifier.modules_to_save.default.bias', 'base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.15.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.3.output.dense.weight', 'base_model.model.bert.encoder.layer.5.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.6.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.14.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.18.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.32.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'base_model.model.rnn.weight_ih_l1_reverse', 'base_model.model.bert.encoder.layer.9.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.output.dense.bias', 'base_model.model.bert.encoder.layer.0.output.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.29.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.28.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.11.output.dense.bias', 'base_model.model.bert.encoder.layer.31.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.11.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.5.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.9.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.15.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.26.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.output.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.12.attention.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.15.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.25.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.12.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.23.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.31.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.31.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.21.output.dense.bias', 'base_model.model.bert.encoder.layer.6.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.17.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.26.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.31.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.1.output.dense.weight', 'base_model.model.bert.encoder.layer.1.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.27.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.30.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.16.attention.self.key.base_layer.weight', 'base_model.model.bert.encoder.layer.24.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.28.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.8.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.output.dense.weight', 'base_model.model.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.9.attention.self.value.base_layer.bias', 'base_model.model.bert.encoder.layer.9.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.31.output.dense.weight', 'base_model.model.bert.encoder.layer.26.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.26.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.19.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.intermediate.dense.bias', 'base_model.model.bert.encoder.layer.20.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.25.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.27.output.dense.weight', 'base_model.model.bert.encoder.layer.18.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.7.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.8.intermediate.dense.weight', 'base_model.model.rnn.bias_hh_l0', 'base_model.model.bert.pooler.dense.weight', 'base_model.model.bert.encoder.layer.20.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.4.attention.self.query.base_layer.bias', 'base_model.model.bert.encoder.layer.31.attention.self.query.lora_A.default.weight', 'base_model.model.bert.encoder.layer.14.attention.self.key.lora_A.default.weight', 'base_model.model.bert.encoder.layer.19.output.dense.bias', 'base_model.model.bert.encoder.layer.6.attention.output.dense.bias', 'base_model.model.bert.encoder.layer.14.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.29.intermediate.dense.weight', 'base_model.model.bert.encoder.layer.22.output.LayerNorm.weight', 'base_model.model.bert.encoder.layer.28.output.LayerNorm.bias', 'base_model.model.bert.encoder.layer.30.attention.self.key.lora_B.default.weight', 'base_model.model.bert.encoder.layer.18.output.dense.bias', 'base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight', 'base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight', 'base_model.model.bert.encoder.layer.22.output.dense.weight', 'base_model.model.bert.encoder.layer.25.attention.self.value.base_layer.weight', 'base_model.model.bert.encoder.layer.8.attention.self.query.base_layer.weight', 'base_model.model.bert.encoder.layer.20.attention.self.key.base_layer.bias', 'base_model.model.bert.encoder.layer.17.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/ and are newly initialized: ['encoder.layer.30.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.30.attention.self.value.bias', 'encoder.layer.13.output.dense.weight', 'encoder.layer.28.output.LayerNorm.bias', 'encoder.layer.13.attention.output.dense.bias', 'encoder.layer.24.output.LayerNorm.weight', 'encoder.layer.28.attention.output.LayerNorm.weight', 'encoder.layer.14.attention.output.LayerNorm.weight', 'encoder.layer.14.output.dense.weight', 'encoder.layer.14.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.18.attention.output.dense.weight', 'encoder.layer.18.attention.self.key.bias', 'encoder.layer.25.attention.self.value.weight', 'encoder.layer.15.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.14.intermediate.dense.bias', 'encoder.layer.17.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.32.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.key.bias', 'encoder.layer.24.attention.output.dense.weight', 'encoder.layer.18.attention.output.LayerNorm.bias', 'encoder.layer.26.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.26.intermediate.dense.weight', 'encoder.layer.26.attention.self.query.bias', 'encoder.layer.21.attention.self.key.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.31.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.26.attention.self.key.weight', 'encoder.layer.21.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.32.attention.self.key.bias', 'encoder.layer.32.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.17.output.LayerNorm.weight', 'encoder.layer.23.attention.self.query.weight', 'encoder.layer.29.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.14.attention.self.value.weight', 'encoder.layer.27.attention.output.dense.weight', 'encoder.layer.20.attention.self.query.weight', 'encoder.layer.31.attention.output.dense.bias', 'encoder.layer.30.output.LayerNorm.weight', 'encoder.layer.23.output.dense.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.23.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.30.attention.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.17.attention.output.dense.weight', 'encoder.layer.32.intermediate.dense.weight', 'encoder.layer.30.attention.output.dense.bias', 'encoder.layer.14.attention.self.key.bias', 'encoder.layer.32.output.dense.bias', 'encoder.layer.22.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.12.output.dense.weight', 'encoder.layer.29.output.dense.bias', 'encoder.layer.18.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.22.intermediate.dense.weight', 'encoder.layer.20.output.dense.bias', 'encoder.layer.15.output.LayerNorm.weight', 'encoder.layer.30.attention.self.query.weight', 'encoder.layer.24.attention.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.30.attention.self.query.bias', 'encoder.layer.26.attention.output.LayerNorm.weight', 'encoder.layer.16.attention.self.key.weight', 'encoder.layer.17.attention.self.key.weight', 'encoder.layer.24.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.25.attention.self.query.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.20.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.24.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.26.output.dense.bias', 'encoder.layer.29.output.LayerNorm.bias', 'encoder.layer.26.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.32.attention.output.dense.weight', 'encoder.layer.13.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.25.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.29.attention.self.query.bias', 'encoder.layer.23.attention.self.query.bias', 'encoder.layer.31.attention.output.LayerNorm.weight', 'encoder.layer.26.attention.self.value.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.27.attention.output.LayerNorm.bias', 'encoder.layer.27.attention.self.query.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.26.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.24.attention.self.key.bias', 'encoder.layer.23.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.value.bias', 'encoder.layer.16.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.19.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.17.attention.self.query.weight', 'encoder.layer.19.output.dense.weight', 'encoder.layer.26.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.28.attention.self.value.bias', 'encoder.layer.14.attention.self.key.weight', 'encoder.layer.22.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.17.attention.self.value.weight', 'encoder.layer.20.attention.self.key.weight', 'encoder.layer.32.output.LayerNorm.weight', 'encoder.layer.29.intermediate.dense.bias', 'encoder.layer.20.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.25.attention.self.value.bias', 'encoder.layer.31.attention.output.dense.weight', 'encoder.layer.28.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.15.output.dense.weight', 'encoder.layer.21.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.15.attention.self.value.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.16.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.15.attention.self.key.bias', 'encoder.layer.32.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.weight', 'encoder.layer.25.intermediate.dense.weight', 'encoder.layer.32.attention.self.value.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.24.attention.self.query.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.13.attention.self.value.weight', 'encoder.layer.25.output.LayerNorm.bias', 'encoder.layer.21.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.19.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.14.attention.output.dense.weight', 'encoder.layer.25.output.dense.weight', 'encoder.layer.28.attention.self.key.weight', 'encoder.layer.28.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.16.attention.self.query.bias', 'encoder.layer.26.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.16.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.29.attention.output.dense.weight', 'encoder.layer.30.intermediate.dense.bias', 'encoder.layer.21.intermediate.dense.weight', 'encoder.layer.12.output.LayerNorm.weight', 'encoder.layer.27.output.LayerNorm.bias', 'encoder.layer.12.intermediate.dense.weight', 'encoder.layer.29.output.dense.weight', 'encoder.layer.12.attention.self.query.weight', 'encoder.layer.26.attention.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.value.weight', 'encoder.layer.19.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.20.attention.output.dense.bias', 'encoder.layer.27.attention.self.key.weight', 'encoder.layer.23.attention.self.value.weight', 'encoder.layer.29.attention.output.LayerNorm.bias', 'encoder.layer.16.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.20.output.dense.weight', 'encoder.layer.18.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.13.attention.self.key.weight', 'encoder.layer.14.output.dense.bias', 'encoder.layer.22.attention.output.dense.weight', 'encoder.layer.20.output.LayerNorm.weight', 'encoder.layer.27.output.dense.weight', 'encoder.layer.21.attention.output.dense.weight', 'encoder.layer.12.attention.self.key.weight', 'encoder.layer.32.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.32.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.weight', 'encoder.layer.22.intermediate.dense.bias', 'encoder.layer.15.intermediate.dense.bias', 'encoder.layer.30.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.31.attention.self.value.weight', 'encoder.layer.27.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.12.intermediate.dense.bias', 'encoder.layer.22.attention.output.dense.bias', 'encoder.layer.22.output.dense.weight', 'encoder.layer.20.attention.output.LayerNorm.bias', 'encoder.layer.27.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.14.output.LayerNorm.bias', 'encoder.layer.16.intermediate.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.18.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.19.intermediate.dense.bias', 'encoder.layer.30.output.dense.bias', 'encoder.layer.12.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.16.attention.self.key.bias', 'encoder.layer.14.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.16.attention.self.value.bias', 'encoder.layer.19.attention.self.query.bias', 'encoder.layer.13.attention.self.query.weight', 'encoder.layer.22.output.LayerNorm.weight', 'encoder.layer.31.attention.self.query.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.28.attention.output.LayerNorm.bias', 'encoder.layer.22.attention.self.key.weight', 'encoder.layer.17.output.LayerNorm.bias', 'encoder.layer.20.attention.output.dense.weight', 'encoder.layer.19.output.dense.bias', 'encoder.layer.14.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.19.attention.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.22.attention.self.query.weight', 'encoder.layer.16.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.31.attention.self.key.bias', 'encoder.layer.29.attention.self.value.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.23.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.18.output.LayerNorm.weight', 'encoder.layer.31.output.LayerNorm.weight', 'encoder.layer.21.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.29.attention.self.key.weight', 'encoder.layer.15.attention.output.LayerNorm.weight', 'encoder.layer.27.intermediate.dense.weight', 'encoder.layer.28.output.LayerNorm.weight', 'encoder.layer.12.attention.self.query.bias', 'encoder.layer.15.attention.self.value.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.18.output.dense.bias', 'encoder.layer.31.attention.self.query.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.25.attention.self.key.bias', 'encoder.layer.29.attention.self.value.weight', 'encoder.layer.12.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.21.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.17.intermediate.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.29.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.15.attention.self.key.weight', 'encoder.layer.15.output.LayerNorm.bias', 'encoder.layer.20.attention.self.value.weight', 'encoder.layer.16.attention.self.value.weight', 'encoder.layer.30.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.18.output.dense.weight', 'encoder.layer.28.attention.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.12.attention.output.dense.weight', 'encoder.layer.30.intermediate.dense.weight', 'encoder.layer.31.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.13.intermediate.dense.weight', 'encoder.layer.19.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.26.attention.self.query.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.27.attention.self.key.bias', 'encoder.layer.14.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.18.attention.self.value.bias', 'encoder.layer.18.intermediate.dense.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.17.output.dense.weight', 'encoder.layer.14.attention.self.value.bias', 'encoder.layer.13.attention.self.key.bias', 'encoder.layer.21.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.16.output.dense.weight', 'encoder.layer.15.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.25.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.28.intermediate.dense.weight', 'classifier.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.24.intermediate.dense.weight', 'encoder.layer.25.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.28.attention.self.value.weight', 'encoder.layer.22.output.LayerNorm.bias', 'encoder.layer.29.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.21.attention.self.value.weight', 'encoder.layer.23.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.21.output.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.17.attention.output.dense.bias', 'encoder.layer.32.attention.self.query.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.24.intermediate.dense.bias', 'encoder.layer.22.attention.output.LayerNorm.weight', 'encoder.layer.31.attention.self.key.weight', 'encoder.layer.25.output.LayerNorm.weight', 'encoder.layer.21.attention.self.query.bias', 'encoder.layer.17.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.24.output.dense.weight', 'encoder.layer.30.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.22.attention.self.value.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.18.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.15.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.21.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.bias', 'encoder.layer.31.output.dense.weight', 'encoder.layer.32.attention.self.key.weight', 'encoder.layer.17.intermediate.dense.bias', 'encoder.layer.25.attention.output.dense.bias', 'encoder.layer.16.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.15.attention.self.query.bias', 'encoder.layer.13.output.LayerNorm.bias', 'encoder.layer.22.attention.output.LayerNorm.bias', 'encoder.layer.27.attention.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.13.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.17.attention.output.LayerNorm.weight', 'encoder.layer.23.attention.self.value.bias', 'encoder.layer.26.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.16.attention.output.dense.bias', 'encoder.layer.24.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.32.attention.output.LayerNorm.weight', 'encoder.layer.21.attention.self.key.weight', 'encoder.layer.18.attention.self.query.weight', 'encoder.layer.12.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.31.intermediate.dense.weight', 'encoder.layer.23.attention.output.LayerNorm.bias', 'encoder.layer.25.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.18.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.19.attention.self.key.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.28.attention.self.query.weight', 'encoder.layer.12.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.30.attention.self.key.weight', 'encoder.layer.25.attention.output.LayerNorm.bias', 'encoder.layer.19.attention.output.LayerNorm.weight', 'encoder.layer.27.attention.self.query.weight', 'encoder.layer.15.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.13.attention.self.query.bias', 'encoder.layer.13.output.dense.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.19.output.LayerNorm.bias', 'encoder.layer.29.attention.self.query.weight', 'encoder.layer.32.attention.self.value.bias', 'encoder.layer.29.intermediate.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.20.attention.self.query.bias', 'encoder.layer.21.output.dense.bias', 'encoder.layer.19.attention.output.dense.bias', 'encoder.layer.31.attention.self.value.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.16.attention.self.query.weight', 'encoder.layer.29.attention.output.dense.bias', 'encoder.layer.25.attention.self.query.weight', 'encoder.layer.12.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.30.attention.self.key.bias', 'encoder.layer.27.attention.self.value.bias', 'encoder.layer.24.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.20.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.12.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.18.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.24.attention.self.key.weight', 'encoder.layer.27.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.12.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.17.attention.self.query.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.20.intermediate.dense.bias', 'encoder.layer.24.attention.self.value.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.19.attention.self.query.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.23.intermediate.dense.weight', 'encoder.layer.27.intermediate.dense.bias', 'encoder.layer.18.attention.output.LayerNorm.weight', 'encoder.layer.15.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.21.attention.output.dense.bias', 'encoder.layer.30.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.14.intermediate.dense.weight', 'encoder.layer.31.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.19.intermediate.dense.weight', 'encoder.layer.24.attention.self.value.bias', 'encoder.layer.16.attention.output.dense.weight', 'encoder.layer.32.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.25.attention.self.key.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'classifier.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.28.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.28.attention.self.query.bias', 'encoder.layer.17.output.dense.bias', 'encoder.layer.26.attention.self.key.bias', 'encoder.layer.22.output.dense.bias', 'encoder.layer.13.attention.output.dense.weight', 'encoder.layer.28.intermediate.dense.bias', 'encoder.layer.23.attention.self.key.bias', 'encoder.layer.31.attention.output.LayerNorm.bias', 'encoder.layer.14.output.LayerNorm.weight', 'encoder.layer.23.output.LayerNorm.bias', 'encoder.layer.20.attention.output.LayerNorm.weight', 'encoder.layer.28.attention.self.key.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.27.output.dense.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.23.attention.output.dense.bias', 'encoder.layer.23.output.LayerNorm.weight', 'encoder.layer.24.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.13.attention.output.LayerNorm.bias', 'encoder.layer.26.output.LayerNorm.bias', 'encoder.layer.20.output.LayerNorm.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/lora_t33_c3_2/checkpoint-150000/\"  # mejor checkpoiunt\n",
    "name_results = \"predictions_esm2_lora_t33_c3\" # \n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)\n",
    "\n",
    "#In case you have added tokens, it’s recommended to use the PeftModel class rather than AutoModelForCausalLM. The former takes into account resizing the embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d08ead",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a350777",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(\"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\", tokenizer_name=\"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D\", max_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2761f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 11, 13, 15, 15, 11, 21, 4, 2, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19, 11,\n",
      "        13, 15, 15, 11, 21,  4,  2,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\ntokenizer \\'input_ids\\': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2],\\ndataloader\\'input_ids\\': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2,  1,  1,  1,  1,  1,  1,  1,  1],\\n\\n \\ntokenizer \\'attention_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\\ndataloader\\'attention_mask\\': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for sample in test_dataset:\n",
    "#    print(sample)\n",
    "#sample = test_dataset[0]\n",
    "#print(sample)\n",
    "#print(sample['input_ids'])\n",
    "#print( next(iter(test_dataset)) )\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[1])\n",
    "\n",
    "#Genera estas diferencia cuando en el tokenizer no definimos que haga un padding \n",
    "\"\"\"\"\n",
    "tokenizer 'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2],\n",
    "dataloader'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2,  1,  1,  1,  1,  1,  1,  1,  1],\n",
    "\n",
    " \n",
    "tokenizer 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "dataloader'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c4961",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4cbbb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/', vocab_size=33, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'})\n",
      "[[{'label': 'LABEL_0', 'score': 0.3711804747581482}, {'label': 'LABEL_1', 'score': 0.628819465637207}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.3667409121990204}, {'label': 'LABEL_1', 'score': 0.6332590579986572}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.3783145546913147}, {'label': 'LABEL_1', 'score': 0.6216854453086853}]]\n",
      "[[{'label': 'LABEL_0', 'score': 0.375162273645401}, {'label': 'LABEL_1', 'score': 0.6248377561569214}]]\n"
     ]
    }
   ],
   "source": [
    "#tokenizer.padding='max_length'\n",
    "#tokenizer.max_length=seq_length\n",
    "print(tokenizer)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\")) # label 1\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\")) # label 1\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYDCEKAFFKM\")) # label 0\n",
    "print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYASDDGSWWD\")) # label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344dc2af",
   "metadata": {},
   "source": [
    "# Predict Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edb1143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils_bert import BertRnn, BertRnnDist\n",
    "from transformers import Trainer, TrainingArguments, BertConfig\n",
    "from transformers import  AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from dataloader_bert import DataSetLoaderBERT\n",
    "from utils import compute_metrics\n",
    "import json\n",
    "from transformers import TextClassificationPipeline, pipeline, BartForSequenceClassification\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "set_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1672458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/M2/ArgosMHC_models/checkpoints/classic_t33_c5/checkpoint-153000/\"  # mejor checkpoiunt\n",
    "name_results = \"predictions_esm2_t33_c5\" # \n",
    "pre_trained = \"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/\"\n",
    "\n",
    "model = BertRnn.from_pretrained(model_name, num_labels=2) # it fail for automodel for sequence classification\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) # it fail for automodel for sequence classification\n",
    "tokenizer = AutoTokenizer.from_pretrained(pre_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b33a1547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 4, 18, 6, 10, 13, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "<class 'dict'>\n",
      "{'input_ids': [0, 19, 18, 5, 20, 19, 16, 9, 17, 20, 5, 21, 11, 13, 5, 17, 11, 4, 19, 12, 12, 19, 10, 13, 19, 11, 22, 7, 5, 10, 7, 19, 10, 6, 19, 11, 13, 15, 15, 11, 21, 4, 2, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19, 11,\n",
      "        13, 15, 15, 11, 21,  4,  2,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "seq_length = 50 # for MHC-I\n",
    "test_dataset = DataSetLoaderBERT(\"/M2/ArgosMHC_models/dataset/hlab/hlab_test_micro.csv\", tokenizer_name=\"/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D\", max_length=seq_length)\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[0])\n",
    "print(type(test_dataset[0]))\n",
    "\n",
    "print( tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\", padding='max_length', max_length=seq_length) )\n",
    "print(test_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "937fe41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([ 0, 19, 18,  5, 20, 19, 16,  9, 17, 20,  5, 21, 11, 13,  5, 17, 11,  4,\n",
      "        19, 12, 12, 19, 10, 13, 19, 11, 22,  7,  5, 10,  7, 19, 10,  6, 19,  4,\n",
      "        18,  6, 10, 13,  4,  2,  1,  1,  1,  1,  1,  1,  1,  1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor(1)}\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m( sample)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;28mtype\u001b[39m(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]) )\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/argosMHC/model_utils_bert.py:270\u001b[0m, in \u001b[0;36mBertRnn.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    267\u001b[0m token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m#print(input_ids.shape) #[batch_size, max_length] = [16, 50]\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m bert_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# lstm\u001b[39;00m\n\u001b[1;32m    282\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m bert_out[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:968\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 968\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[1;32m    969\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# el tokenizer devuelve una lsita, falta convertirlos a tensores\n",
    "# sample = tokenizer(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\", padding='max_length', max_length=seq_length)\n",
    "sample = test_dataset[0]\n",
    "print( sample)\n",
    "print( type(sample['input_ids']) )\n",
    "output = model( sample['input_ids'], sample['attention_mask'] )\n",
    "prediction = torch.argmax(output)\n",
    "\n",
    "\n",
    "# genera errores raros, vamos a ver cual es la entrada al forward del modelo, al entrenar y comprar con eso.\n",
    "# luego de eso degería salir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f402af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vicente/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "The model 'BertModel' is not supported for . Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'EsmForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegatronBertForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='/M2/ArgosMHC_models/pre_trained_models/esm2_t33_650M_UR50D/', vocab_size=33, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'})\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m TextClassificationPipeline(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, return_all_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#pipe = pipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# label 1\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\")) # label 1\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYDCEKAFFKM\")) # label 0\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYASDDGSWWD\")) # label 0\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:140\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/base.py:1063\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1062\u001b[0m     )\n\u001b[0;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:115\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    114\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 115\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:184\u001b[0m, in \u001b[0;36mTextClassificationPipeline.postprocess\u001b[0;34m(self, model_outputs, function_to_apply, top_k, _legacy)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         function_to_apply \u001b[38;5;241m=\u001b[39m ClassificationFunction\u001b[38;5;241m.\u001b[39mNONE\n\u001b[0;32m--> 184\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m function_to_apply \u001b[38;5;241m==\u001b[39m ClassificationFunction\u001b[38;5;241m.\u001b[39mSIGMOID:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch11/lib/python3.9/site-packages/transformers/utils/generic.py:263\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    262\u001b[0m     inner_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m (k, v) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tuple()[k]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'logits'"
     ]
    }
   ],
   "source": [
    "#tokenizer.padding='max_length'\n",
    "#tokenizer.max_length=seq_length\n",
    "print(tokenizer)\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "#pipe = pipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "print(pipe([\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYLFGRDL\"])) # label 1\n",
    "#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYTDKKTHL\")) # label 1\n",
    "#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYDCEKAFFKM\")) # label 0\n",
    "#print(pipe(\"YFAMYQENMAHTDANTLYIIYRDYTWVARVYRGYASDDGSWWD\")) # label 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
