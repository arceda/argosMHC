{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee4237d-bb13-476a-a7c5-f7b9a3ddc40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Comandos previos\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f61c8c5-cac6-468b-8ad6-a90870b66e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831a3a3238194a6d87e66e9389c0ba92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import compute_metrics\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e020c4-54a7-4251-9490-b9abe142c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-1767aa7e0d445794.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-e14c433f0c95fb3c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-c7b1add269362cb1.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6584f0d-0ae4-4bf9-bc2b-55047b419cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-89968d56dfd03f35.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-8a52e5d1975fcd86.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f3a3aa-9c41-45ff-a303-86ab63d076c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PEFT: Parametros de ajuste fino eficiente\n",
    "# task_type: Especifica el tipo de tarea el cual  el modelo se ajustara\n",
    "# r son las dimensiones de las matrices A y B\n",
    "# lora_alpha es el factor de escala, determina la relacion de los pesos A y B\n",
    "# en relacion con los parametros originales del modelo\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8023c479-93cf-4ff8-bce7-6bd894f01649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertamos las matrices A y B en el modelo (get_peft_model)\n",
    "from peft import get_peft_model\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa9d070-5f08-4fb6-8117-f405c8d32560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO Y EVALUACION DEL MODELO\n",
    "# evaluate.load calcula e informa metricas, es una fx de precision sencilla\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30740c7-40a0-4a70-b079-54bf40335dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    prediction=pred.predictions\n",
    "    preds = prediction.argmax(-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    precision = tp / (tp + fp) \n",
    "    recall = tp / (tp + fn)\n",
    "    sn = tp / (tp + fp)       \n",
    "    sp = tn / (tn + fp)  # true negative rate\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'sn': sn,\n",
    "        'sp': sp,\n",
    "        'accuracy': acc,\n",
    "        'mcc': mcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6c8df1-8358-488f-a34e-4efd648142ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n",
    "                                 num_train_epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b623d-cf57-4a06-99f7-7f93aa8f4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5719cbb9-79cb-447c-aa7b-5c2d259a3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 10:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Sn</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696316</td>\n",
       "      <td>0.493340</td>\n",
       "      <td>0.484425</td>\n",
       "      <td>0.924180</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.484425</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>-0.026269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685058</td>\n",
       "      <td>0.539687</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.151639</td>\n",
       "      <td>0.247078</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.927734</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.126302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.679707</td>\n",
       "      <td>0.541016</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.213660</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.957031</td>\n",
       "      <td>0.551000</td>\n",
       "      <td>0.148628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.696100</td>\n",
       "      <td>0.673374</td>\n",
       "      <td>0.586658</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.280738</td>\n",
       "      <td>0.402941</td>\n",
       "      <td>0.713542</td>\n",
       "      <td>0.892578</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.219951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.696100</td>\n",
       "      <td>0.665103</td>\n",
       "      <td>0.638448</td>\n",
       "      <td>0.713355</td>\n",
       "      <td>0.448770</td>\n",
       "      <td>0.550943</td>\n",
       "      <td>0.713355</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.300072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.696100</td>\n",
       "      <td>0.652674</td>\n",
       "      <td>0.653272</td>\n",
       "      <td>0.712610</td>\n",
       "      <td>0.497951</td>\n",
       "      <td>0.586248</td>\n",
       "      <td>0.712610</td>\n",
       "      <td>0.808594</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>0.323235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.696100</td>\n",
       "      <td>0.634150</td>\n",
       "      <td>0.653544</td>\n",
       "      <td>0.773234</td>\n",
       "      <td>0.426230</td>\n",
       "      <td>0.549538</td>\n",
       "      <td>0.773234</td>\n",
       "      <td>0.880859</td>\n",
       "      <td>0.659000</td>\n",
       "      <td>0.346158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.599129</td>\n",
       "      <td>0.711738</td>\n",
       "      <td>0.706612</td>\n",
       "      <td>0.700820</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.706612</td>\n",
       "      <td>0.722656</td>\n",
       "      <td>0.712000</td>\n",
       "      <td>0.423571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.563566</td>\n",
       "      <td>0.724545</td>\n",
       "      <td>0.746544</td>\n",
       "      <td>0.663934</td>\n",
       "      <td>0.702820</td>\n",
       "      <td>0.746544</td>\n",
       "      <td>0.785156</td>\n",
       "      <td>0.726000</td>\n",
       "      <td>0.452925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.531163</td>\n",
       "      <td>0.739658</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.725410</td>\n",
       "      <td>0.731405</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.753906</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.479562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.506694</td>\n",
       "      <td>0.753778</td>\n",
       "      <td>0.774266</td>\n",
       "      <td>0.702869</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.774266</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.510740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.485079</td>\n",
       "      <td>0.761879</td>\n",
       "      <td>0.780761</td>\n",
       "      <td>0.715164</td>\n",
       "      <td>0.746524</td>\n",
       "      <td>0.780761</td>\n",
       "      <td>0.808594</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.526573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.469682</td>\n",
       "      <td>0.761735</td>\n",
       "      <td>0.784580</td>\n",
       "      <td>0.709016</td>\n",
       "      <td>0.744887</td>\n",
       "      <td>0.784580</td>\n",
       "      <td>0.814453</td>\n",
       "      <td>0.763000</td>\n",
       "      <td>0.527001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.456003</td>\n",
       "      <td>0.767930</td>\n",
       "      <td>0.786192</td>\n",
       "      <td>0.723361</td>\n",
       "      <td>0.753469</td>\n",
       "      <td>0.786192</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.769000</td>\n",
       "      <td>0.538515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.557100</td>\n",
       "      <td>0.444632</td>\n",
       "      <td>0.786565</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.768443</td>\n",
       "      <td>0.778816</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.573683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.464800</td>\n",
       "      <td>0.436994</td>\n",
       "      <td>0.792969</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.780384</td>\n",
       "      <td>0.813333</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.794000</td>\n",
       "      <td>0.588720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.464800</td>\n",
       "      <td>0.430888</td>\n",
       "      <td>0.795642</td>\n",
       "      <td>0.797071</td>\n",
       "      <td>0.780738</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.797071</td>\n",
       "      <td>0.810547</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.591687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.464800</td>\n",
       "      <td>0.427364</td>\n",
       "      <td>0.799404</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.774590</td>\n",
       "      <td>0.790795</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.824219</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.599866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.464800</td>\n",
       "      <td>0.425576</td>\n",
       "      <td>0.798284</td>\n",
       "      <td>0.809935</td>\n",
       "      <td>0.768443</td>\n",
       "      <td>0.788644</td>\n",
       "      <td>0.809935</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.598035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.440400</td>\n",
       "      <td>0.424878</td>\n",
       "      <td>0.797403</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.772541</td>\n",
       "      <td>0.788703</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.822266</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.595857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "/usr/local/lib/python3.9/dist-packages/wandb/wandb_torch.py:193: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  check = torch.cuda.FloatTensor(1).fill_(0)\n",
      "Saving model checkpoint to test_trainer/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PeftModelForSequenceClassification.forward` and have been ignored: text, token_type_ids. If text, token_type_ids are not expected by `PeftModelForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./BERT_LoRA\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./BERT_LoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81779ab-bf83-4566-9f5f-dfe47933aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
