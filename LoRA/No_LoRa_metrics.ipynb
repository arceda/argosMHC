{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee4237d-bb13-476a-a7c5-f7b9a3ddc40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Comandos previos\n",
    "!pip install -q transformers\n",
    "!pip install -q peft\n",
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f61c8c5-cac6-468b-8ad6-a90870b66e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e9fbbec92640dd944b3aa633d394b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import compute_metrics\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e020c4-54a7-4251-9490-b9abe142c6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-1767aa7e0d445794.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-e14c433f0c95fb3c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-c7b1add269362cb1.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6584f0d-0ae4-4bf9-bc2b-55047b419cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-89968d56dfd03f35.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-8a52e5d1975fcd86.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f3a3aa-9c41-45ff-a303-86ab63d076c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# PEFT: Parametros de ajuste fino eficiente\n",
    "# task_type: Especifica el tipo de tarea el cual  el modelo se ajustara\n",
    "# r son las dimensiones de las matrices A y B\n",
    "# lora_alpha es el factor de escala, determina la relacion de los pesos A y B\n",
    "# en relacion con los parametros originales del modelo\n",
    "#from peft import LoraConfig, TaskType\n",
    "\n",
    "#lora_config = LoraConfig(\n",
    "#    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n",
    "#)\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8023c479-93cf-4ff8-bce7-6bd894f01649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insertamos las matrices A y B en el modelo (get_peft_model)\n",
    "#from peft import get_peft_model\n",
    "#model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa9d070-5f08-4fb6-8117-f405c8d32560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAMIENTO Y EVALUACION DEL MODELO\n",
    "# evaluate.load calcula e informa metricas, es una fx de precision sencilla\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30740c7-40a0-4a70-b079-54bf40335dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILS\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    prediction=pred.predictions\n",
    "    preds = prediction.argmax(-1)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    precision = tp / (tp + fp) \n",
    "    recall = tp / (tp + fn)\n",
    "    sn = tp / (tp + fp)       \n",
    "    sp = tn / (tn + fp)  # true negative rate\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'sn': sn,\n",
    "        'sp': sp,\n",
    "        'accuracy': acc,\n",
    "        'mcc': mcc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6c8df1-8358-488f-a34e-4efd648142ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",\n",
    "                                 num_train_epochs=20,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083b623d-cf57-4a06-99f7-7f93aa8f4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5719cbb9-79cb-447c-aa7b-5c2d259a3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 11:51, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Sn</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.453806</td>\n",
       "      <td>0.826316</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.756148</td>\n",
       "      <td>0.810989</td>\n",
       "      <td>0.874408</td>\n",
       "      <td>0.896484</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.660531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.674507</td>\n",
       "      <td>0.776575</td>\n",
       "      <td>0.690058</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.805461</td>\n",
       "      <td>0.690058</td>\n",
       "      <td>0.585938</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>0.594726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669643</td>\n",
       "      <td>0.844935</td>\n",
       "      <td>0.840491</td>\n",
       "      <td>0.842213</td>\n",
       "      <td>0.841351</td>\n",
       "      <td>0.840491</td>\n",
       "      <td>0.847656</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.689838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.803383</td>\n",
       "      <td>0.847016</td>\n",
       "      <td>0.911980</td>\n",
       "      <td>0.764344</td>\n",
       "      <td>0.831661</td>\n",
       "      <td>0.911980</td>\n",
       "      <td>0.929688</td>\n",
       "      <td>0.849000</td>\n",
       "      <td>0.705617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.671210</td>\n",
       "      <td>0.876121</td>\n",
       "      <td>0.866935</td>\n",
       "      <td>0.881148</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.866935</td>\n",
       "      <td>0.871094</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>0.752049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.876103</td>\n",
       "      <td>0.864594</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>0.889344</td>\n",
       "      <td>0.864542</td>\n",
       "      <td>0.841085</td>\n",
       "      <td>0.839844</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.729351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.312200</td>\n",
       "      <td>0.867168</td>\n",
       "      <td>0.874071</td>\n",
       "      <td>0.866397</td>\n",
       "      <td>0.877049</td>\n",
       "      <td>0.871690</td>\n",
       "      <td>0.866397</td>\n",
       "      <td>0.871094</td>\n",
       "      <td>0.874000</td>\n",
       "      <td>0.747981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>1.013965</td>\n",
       "      <td>0.856285</td>\n",
       "      <td>0.814679</td>\n",
       "      <td>0.909836</td>\n",
       "      <td>0.859632</td>\n",
       "      <td>0.814679</td>\n",
       "      <td>0.802734</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.715268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>1.034229</td>\n",
       "      <td>0.861216</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.911885</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.810547</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.724786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.877601</td>\n",
       "      <td>0.870021</td>\n",
       "      <td>0.863821</td>\n",
       "      <td>0.870902</td>\n",
       "      <td>0.867347</td>\n",
       "      <td>0.863821</td>\n",
       "      <td>0.869141</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.739924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.995977</td>\n",
       "      <td>0.859951</td>\n",
       "      <td>0.826742</td>\n",
       "      <td>0.899590</td>\n",
       "      <td>0.861629</td>\n",
       "      <td>0.826742</td>\n",
       "      <td>0.820312</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.721083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>1.018706</td>\n",
       "      <td>0.842581</td>\n",
       "      <td>0.925831</td>\n",
       "      <td>0.741803</td>\n",
       "      <td>0.823663</td>\n",
       "      <td>0.925831</td>\n",
       "      <td>0.943359</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.701846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.893317</td>\n",
       "      <td>0.867139</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.872951</td>\n",
       "      <td>0.864975</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.861328</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.734081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.969018</td>\n",
       "      <td>0.872326</td>\n",
       "      <td>0.889849</td>\n",
       "      <td>0.844262</td>\n",
       "      <td>0.866456</td>\n",
       "      <td>0.889849</td>\n",
       "      <td>0.900391</td>\n",
       "      <td>0.873000</td>\n",
       "      <td>0.746485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>1.013861</td>\n",
       "      <td>0.869829</td>\n",
       "      <td>0.869835</td>\n",
       "      <td>0.862705</td>\n",
       "      <td>0.866255</td>\n",
       "      <td>0.869835</td>\n",
       "      <td>0.876953</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.739824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.036416</td>\n",
       "      <td>0.865971</td>\n",
       "      <td>0.861224</td>\n",
       "      <td>0.864754</td>\n",
       "      <td>0.862986</td>\n",
       "      <td>0.861224</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.731877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.048708</td>\n",
       "      <td>0.867091</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.870902</td>\n",
       "      <td>0.864700</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>0.734008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.056363</td>\n",
       "      <td>0.869141</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.867005</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.738082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>1.061366</td>\n",
       "      <td>0.869141</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.867005</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.738082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.062974</td>\n",
       "      <td>0.869141</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.867005</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.738082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "/usr/local/lib/python3.9/dist-packages/wandb/wandb_torch.py:193: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  check = torch.cuda.FloatTensor(1).fill_(0)\n",
      "Saving model checkpoint to test_trainer/checkpoint-500\n",
      "Configuration saved in test_trainer/checkpoint-500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1000\n",
      "Configuration saved in test_trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-1500\n",
      "Configuration saved in test_trainer/checkpoint-1500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2000\n",
      "Configuration saved in test_trainer/checkpoint-2000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-2500\n",
      "Configuration saved in test_trainer/checkpoint-2500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-2500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./BERT_NoLoRA\n",
      "Configuration saved in ./BERT_NoLoRA/config.json\n",
      "Model weights saved in ./BERT_NoLoRA/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./BERT_NoLoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81779ab-bf83-4566-9f5f-dfe47933aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
